<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Linear_Algebra</title>
  <style>
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<p>[TOC]</p>
<h1 id="linear-algebra">Linear Algebra</h1>
<h2 id="analytic-geometry">Analytic Geometry</h2>
<h3 id="norms">Norms</h3>
<ul>
<li>A norm on a vector space V is a function which assigns each vector x its length such that for all <span class="math inline">\(\lambda \in \R\)</span> and x, y in V the following hold:
<ul>
<li>Absolutely homogeneous: <span class="math inline">\(\|\lambda \boldsymbol{x}\|=| \lambda\|\boldsymbol{x}\|\)</span></li>
<li>Triangle Inequality (the sum of the lengths of any two sides must be greater than or equal to the length of the remaining side): <span class="math inline">\(\|\boldsymbol{x}+\boldsymbol{y}\| \leqslant\|\boldsymbol{x}\|+\|\boldsymbol{y}\|\)</span></li>
<li>Positive Definite: <span class="math inline">\(\|x\| \geqslant 0 \text { and }\|x\|=0 \Longleftrightarrow x=0\)</span></li>
</ul></li>
<li><span class="math inline">\(\ell_1\)</span> norm (Manhattan norm): <span class="math inline">\(\|x\|_{1}:=\sum_{i=1}^{n}\left|x_{i}\right|\)</span></li>
<li><span class="math inline">\(\ell_2\)</span> norm (euclidean norm): <span class="math inline">\(\|x\|_{2}:=\sqrt{\sum_{i=1}^{n} x_{i}^{2}}=\sqrt{x^{\top} x}\)</span>
<ul>
<li>euclidean distance of the vector</li>
</ul></li>
<li>Frobenius norm of a matrix: <span class="math inline">\(\|X\|_{F}=\sqrt{\sum_{i=1}^{M} \sum_{j=1}^{N} X_{i j}^{2}}=\sqrt{\sum_{i=1}^{m} \sum_{j=1}^{n}\left|a_{i j}\right|^{2}}=\sqrt{\operatorname{trace}\left(A A^H\right)}=\sqrt{\sum_{i=1}^{\min \{m, n\}} \sigma_{i}^{2}(A)}\)</span> for sigma, the singular values of A</li>
</ul>
<h3 id="inner-products">Inner Products</h3>
<ul>
<li>Dot product: <span class="math inline">\(x \cdot y = \boldsymbol{x}^{\top} \boldsymbol{y}=\sum_{i=1}^{n} x_{i} y_{i}\)</span></li>
<li>General Inner products
<ul>
<li>bilinear mapping <span class="math inline">\(\Omega\)</span> is a mapping with 2 arguments and linear in each argument: <span class="math inline">\(\Omega(\lambda \boldsymbol{x}+\psi \boldsymbol{y}, \boldsymbol{z})=\lambda \Omega(\boldsymbol{x}, \boldsymbol{z})+\psi \Omega(\boldsymbol{y}, \boldsymbol{z})\)</span></li>
<li>For <span class="math inline">\(\Omega: V \times V \rightarrow \mathbf{R}\)</span>, mapping is symmetric if <span class="math inline">\(\Omega(x, y)=\Omega(y, x)\)</span></li>
<li>Mapping is positive definite if <span class="math inline">\(\forall x \in V \backslash\{0\}: \Omega(x, x)&gt;0, \quad \Omega(0,0)=0\)</span></li>
<li>A positive definite, symmetric bilinear mapping is called an inner product on V, denoted <span class="math inline">\(\langle\boldsymbol{x}, \boldsymbol{y}\rangle\)</span></li>
</ul></li>
<li>Given a basis B, can write x, y in terms of that basis. Then <span class="math inline">\(\langle\boldsymbol{x}, \boldsymbol{y}\rangle=\left\langle\sum_{i=1}^{n} \psi_{i} \boldsymbol{b}_{i}, \sum_{j=1}^{n} \lambda_{j} \boldsymbol{b}_{j}\right\rangle=\sum_{i=1}^{n} \sum_{j=1}^{n} \psi_{i}\left\langle\boldsymbol{b}_{i}, \boldsymbol{b}_{j}\right\rangle \lambda_{j}=\hat{\boldsymbol{x}}^{\top} \boldsymbol{A} \hat{\boldsymbol{y}}\)</span> where <span class="math inline">\(A_{i j}:=\left\langle\boldsymbol{b}_{i}, \boldsymbol{b}_{j}\right\rangle\)</span> and <span class="math inline">\(\hat{\boldsymbol{x}}, \hat{\boldsymbol{y}}\)</span> are the coordinates wrt the basis. The inner product is uniquely determine by A.</li>
</ul>
<h3 id="outer-products">Outer Products</h3>
<ul>
<li>In matrix multiplication, can be done by taking the columns of A times rows of B to get AB</li>
<li>one column u times one row <span class="math inline">\(v^T\)</span> produces a matrix. While inner product <span class="math inline">\(v^Tu\)</span> produces a scalar, outer product produces <span class="math inline">\(uv^T = \left[\begin{array}{c}2\\2\\1\end{array}\right]\left[\begin{array}{c}3&amp;4&amp;6\end{array}\right] = \left[\begin{array}{c}6&amp;8&amp;12 \\6&amp;8&amp;12\\3&amp;4&amp;6\end{array}\right]\)</span> a rank 1 matrix.</li>
<li>The column space of the outer product is one-dimensional - the line in the direction of u. The row space is the line through v</li>
<li><span class="math inline">\((uv^T)^T = vu^T\)</span></li>
</ul>
<h3 id="lengths-and-distances">Lengths and Distances</h3>
<ul>
<li>Inner products and norms are closely related in the sense that any inner product induces a norm in a natural way, such that we can compute lengths of vectors using the inner product. However, not every norm is induced by an inner product. The Manhattan norm (3.3) is an example of a norm without a corresponding inner product.</li>
<li>Length <span class="math inline">\(\|x\|:=\sqrt{\langle x, x\rangle} = \sqrt{x \cdot x}\)</span></li>
<li>Schwarz Inequality: <span class="math inline">\(|\boldsymbol{v} \cdot \boldsymbol{w}| \leq\|\boldsymbol{v}\|\|\boldsymbol{w}\|\)</span></li>
<li>Triangle Inequality: <span class="math inline">\(\|\boldsymbol{v}+\boldsymbol{w}\| \leq\|\boldsymbol{v}\|+\|\boldsymbol{w}\|\)</span></li>
<li>Distance between x and y: <span class="math inline">\(d(\boldsymbol{x}, \boldsymbol{y}):=\|\boldsymbol{x}-\boldsymbol{y}\|=\sqrt{\langle\boldsymbol{x}-\boldsymbol{y}, \boldsymbol{x}-\boldsymbol{y}\rangle}\)</span>. If we use the dot product as the inner product, then we get the euclidean distance.</li>
<li>A metric d satisfies: symmetric, positive definite, triangle inequality (<span class="math inline">\(d(\boldsymbol{x}, \boldsymbol{z}) \leqslant d(\boldsymbol{x}, \boldsymbol{y})+d(\boldsymbol{y}, \boldsymbol{z})\)</span>).</li>
</ul>
<h3 id="angles">Angles</h3>
<ul>
<li>Inner products capture the geometry of a vector space by defining the angle <span class="math inline">\(\omega\)</span> between two vectors.</li>
<li>From Cauchy-Schwarz Inequality, <span class="math inline">\(-1 \leqslant \frac{\langle\boldsymbol{x}, \boldsymbol{y}\rangle}{\|\boldsymbol{x}\|\|\boldsymbol{y}\|} \leqslant 1\)</span> for $x,y 0 $. Then there exists a unique <span class="math inline">\(\omega \in[0, \pi], \; \cos \omega=\frac{\langle\boldsymbol{x}, \boldsymbol{y}\rangle}{\|\boldsymbol{x}\|\|\boldsymbol{y}\|}\)</span>. Using the dot product as the inner product, this translates to <span class="math inline">\(\cos \omega=\frac{\langle\boldsymbol{x}, \boldsymbol{y}\rangle}{\sqrt{\langle\boldsymbol{x}, \boldsymbol{x}\rangle\langle\boldsymbol{y}, \boldsymbol{y}\rangle}}=\frac{\boldsymbol{x}^{\top} \boldsymbol{y}}{\sqrt{\boldsymbol{x}^{\top} \boldsymbol{x} \boldsymbol{y}^{\top} \boldsymbol{y}}}\)</span></li>
<li>Orthogonality: <span class="math inline">\(\langle x, y\rangle = 0 \implies x \perp y\)</span>. Orthonormal when <span class="math inline">\(||x|| = 1 = ||y||\)</span>. Can be orthogonal wrt one inner product but not another</li>
</ul>
<h3 id="inner-products-of-functions">Inner Products of Functions</h3>
<ul>
<li><span class="math inline">\(\langle u, v\rangle:=\int_{a}^{b} u(x) v(x) d x\)</span> for limits <span class="math inline">\(a, b&lt;\infty\)</span>. If this evaluates to 0, functions u and v are orthogonal.</li>
<li>Unlike inner products on finite-dimensional vectors, inner products on functions may diverge</li>
</ul>
<h2 id="vectors-and-matrices">Vectors and Matrices</h2>
<ul>
<li>Linear combination: <span class="math inline">\(c v+d w=c\left[\begin{array}{l} {1} \\ {1} \end{array}\right]+d\left[\begin{array}{l} {2} \\ {3} \end{array}\right]=\left[\begin{array}{c} {c+2 d} \\ {c+3 d} \end{array}\right]\)</span></li>
<li>Dot product: <span class="math inline">\(\boldsymbol{v} \cdot \boldsymbol{w}=v_{1} w_{1}+v_{2} w_{2}\)</span>.</li>
<li>Dot product = 0 indicates orthogonality: <span class="math inline">\(v \cdot w = 0 \iff v \perp w\)</span></li>
</ul>
<h3 id="matrix-x-vector-multiplication">Matrix x Vector Multiplication</h3>
<ul>
<li>Row multiplication: <span class="math inline">\(A x=\left[\begin{array}{ll} {(\operatorname{row} I)} &amp; {\cdot x} \\ {(\operatorname{row} 2)} &amp; {\cdot x} \\ {(\operatorname{row} 3)} &amp; {x} \end{array}\right]\)</span>
<ul>
<li>Example: <span class="math inline">\(A \boldsymbol{x}=\left[\begin{array}{rrr} {1} &amp; {0} &amp; {0} \\ {-1} &amp; {1} &amp; {0} \\ {0} &amp; {-1} &amp; {1} \end{array}\right]\left[\begin{array}{l} {x_{1}} \\ {x_{2}} \\ {x_{3}} \end{array}\right]=\left[\begin{array}{c} {(1,0,0) \cdot\left(x_{1}, x_{2}, x_{3}\right)} \\ {(-1,1,0) \cdot\left(x_{1}, x_{2}, x_{3}\right)} \\ {(0,-1,1) \cdot\left(x_{1}, x_{2}, x_{3}\right)} \end{array}\right]\)</span></li>
</ul></li>
<li>Column multiplication: <span class="math inline">\(A x=x(\operatorname{column} 1)+y(\operatorname{column} 2)+z(\operatorname{column} 3)\)</span>
<ul>
<li>Example: $Ax = =x_1+ x_2$</li>
</ul></li>
</ul>
<h3 id="matrix-multiplication">Matrix Multiplication</h3>
<ul>
<li>Matrix multiplication is associative and distributive but not commutative: (AB)C = A(BC)</li>
<li>To multiply AB, if A has n columns, B must have n rows. Left columns = Right rows</li>
</ul>
<ol type="1">
<li>Dot product each row in A with column in B
<ul>
<li>The entry in row i and column j of AB is (row i of A) Â· ( column j of B)</li>
</ul></li>
<li>Matrix A times every column of B
<ul>
<li>Each column of AB is a combination of the columns of A.</li>
<li><span class="math inline">\(A\left[b_{1} \cdots b_{p}\right]=\left[A b_{1} \cdots A b_{p}\right]\)</span></li>
</ul></li>
<li>Every row of A times matrix B
<ul>
<li>Each row of AB is a combination of the rows of B.</li>
<li><span class="math inline">\(\left[\begin{array}{cc} {\operatorname{row} i} &amp; {\text { of } A} \end{array}\right]\left[\begin{array}{ccc} {1} &amp; {2} &amp; {3} \\ {4} &amp; {5} &amp; {6} \\ {7} &amp; {8} &amp; {9} \end{array}\right]=\left[\begin{array}{cc} {\operatorname{row} i} &amp; {\text { of } A B} \end{array}\right]\)</span></li>
</ul></li>
<li>Multiply columns 1 ton of A times rows 1 ton of B. Add those matrices.
<ul>
<li>Column 1 of A multiplies row 1 of B. Columns 2 and 3 multiply rows 2 and 3.</li>
<li><span class="math inline">\(\left[\begin{array}{ccc} {\operatorname{col} 1} &amp; {\operatorname{col} 2} &amp; {\operatorname{col} 3} \\ {\cdot} &amp; {\cdot} &amp; {\cdot} \\ {\cdot} &amp; {\cdot} &amp; {\cdot} \end{array}\right]\left[\begin{array}{cc} {\operatorname{row} 1} &amp; {\cdots} \\ {\operatorname{row} 2} &amp; {\cdots} \\ {\operatorname{row} 3} &amp; {\cdots} \end{array}\right]=(\operatorname{col} 1)(\operatorname{row} 1)+(\operatorname{col} 2)(\operatorname{row} 2)+(\operatorname{col} 3)(\operatorname{row} 3)\)</span></li>
<li><span class="math inline">\(A B=\left[\begin{array}{l} {a} \\ {c} \end{array}\right]\left[\begin{array}{ll} {E} &amp; {F} \end{array}\right]+\left[\begin{array}{l} {b} \\ {d} \end{array}\right]\left[\begin{array}{ll} {G} &amp; {H} \end{array}\right] = \left[\begin{array}{cc} {\boldsymbol{a} \boldsymbol{E}+b G} &amp; {\boldsymbol{a} \boldsymbol{F}+b H} \\ {\boldsymbol{c} \boldsymbol{E}+d G} &amp; {\boldsymbol{c} \boldsymbol{F}+d H} \end{array}\right]\)</span></li>
</ul></li>
</ol>
<ul>
<li>Block multiplication - if blocks of A are the right size to multiply blocks of B, can divide into smaller matrices and multiply
<ul>
<li><span class="math inline">\(\left[\begin{array}{ll} {A_{11}} &amp; {A_{12}} \\ {A_{21}} &amp; {A_{22}} \end{array}\right]\left[\begin{array}{l} {B_{11}} \\ {B_{21}} \end{array}\right]=\left[\begin{array}{l} {A_{11} B_{11}+A_{12} B_{21}} \\ {A_{21} B_{11}+A_{22} B_{21}} \end{array}\right]\)</span></li>
</ul></li>
</ul>
<h3 id="inverses">Inverses</h3>
<ul>
<li><span class="math inline">\(A^{-1}A = I = AA^{-1}\)</span> if the inverse exists r = m = n</li>
<li>For square matrices, a left inverse = right inverse</li>
<li>Easiest test for invertibility: If singular, det = 0, inverse does not exist</li>
<li><span class="math inline">\((\boldsymbol{A} \boldsymbol{B})^{-1}=\boldsymbol{B}^{-1} \boldsymbol{A}^{-1}\)</span></li>
</ul>
<h4 id="left-inverse">Left Inverse</h4>
<ul>
<li>Matrix with full column rank r = n, N(A) = {0}, independent columns, 0 or 1 solutions to Ax = b</li>
<li><span class="math inline">\((A^TA)^{-1}A^TA = I\)</span> so the left inverse is <span class="math inline">\((A^TA)^{-1}A^T\)</span> (Moore-Penrose Pseudoinverse)</li>
<li><span class="math inline">\(AA_{left}^{-1}\)</span> is then a projection onto the column space</li>
<li>Used in least squares since we sub in <span class="math inline">\(\boldsymbol{x}=\left(\boldsymbol{A}^{\top} \boldsymbol{A}\right)^{-1} \boldsymbol{A}^{\top} \boldsymbol{b}\)</span> for <span class="math inline">\(\boldsymbol{x}=A^{-1} \boldsymbol{b}\)</span></li>
</ul>
<h4 id="right-inverse">Right Inverse</h4>
<ul>
<li>Matrix with full row rank r = m, <span class="math inline">\(N(A^T) = \{0\}\)</span> independent rows, infinite solutions to Ax = b, n - m free variables</li>
<li><span class="math inline">\(AA^T(AA^T)^{-1}= I\)</span> so the right inverse is <span class="math inline">\(A^T(AA^T)^{-1}\)</span></li>
<li><span class="math inline">\(A_{right}^{-1}A\)</span> is then a projection onto the row space</li>
</ul>
<h4 id="pseudoinverse">Pseudoinverse</h4>
<ul>
<li><span class="math inline">\(y = A^+(Ay)\)</span></li>
<li>If x, y both in the row space, then <span class="math inline">\(Ax \neq Ay\)</span> - there is a one to one mapping from x to Ax.</li>
<li>Pseudoinverse often used in stats since least squares matrices often are not of full rank</li>
<li>Finding pseudoinverse
<ul>
<li>SVD: <span class="math inline">\(A = U\Sigma V^T\)</span></li>
<li>Here <span class="math inline">\(\Sigma = \left[\begin{array}{ccc}\sigma_1 &amp; 0 &amp; ...\\ ... &amp; \sigma_r &amp; ... \\ 0 &amp; 0 &amp; 0 \end{array}\right]\)</span> has rank r, n x m</li>
<li><span class="math inline">\(\Sigma^{+} = \left[\begin{array}{ccc}1/\sigma_1 &amp; 0 &amp; ...\\ ... &amp; 1/\sigma_r &amp; ... \\ 0 &amp; 0 &amp; 0 \end{array}\right]\)</span> n x m, rank r</li>
<li>Then <span class="math inline">\(A^+ = V\Sigma^+ U^T\)</span></li>
</ul></li>
<li>Note that <span class="math inline">\(\Sigma \Sigma^+\)</span> is m x m with diagonal ones and is a projection onto row space, <span class="math inline">\(\Sigma^+ \Sigma\)</span> an n x n matrix projecting onto column space.</li>
</ul>
<h3 id="permutations">Permutations</h3>
<ul>
<li>A matrix that executes row exchanges - this may be needed to make a matrix invertible. PA = LU</li>
<li>P = identity matrix with reordered rows. n! possible matrices / reorderings</li>
<li>For invertible P, <span class="math inline">\(P^{-1} = P^T\)</span></li>
</ul>
<h3 id="transposes">Transposes</h3>
<ul>
<li>Switch columns and rows - <span class="math inline">\((A^T)_{ij} = A_{ji}\)</span></li>
<li>For symmetric matrices, <span class="math inline">\(A^T= A\)</span>.</li>
<li>Note <span class="math inline">\(A^TA\)</span> is always symmetric, square.</li>
<li><span class="math inline">\(\begin{aligned} (\boldsymbol{A}+\boldsymbol{B})^{\top} &amp;=\boldsymbol{A}^{\top}+\boldsymbol{B}^{\top} \\ (\boldsymbol{A} \boldsymbol{B})^{\top} &amp;=\boldsymbol{B}^{\top} \boldsymbol{A}^{\top} \end{aligned}\)</span></li>
</ul>
<h3 id="echelon-form">Echelon Form</h3>
<ul>
<li>Staircase from top corner separating values from 0âs, ie. U or L</li>
<li>R = reduced row echelon form. <span class="math inline">\(R = \left[\begin{array}{rrr} {I} &amp; {F} \\ {0} &amp; {0} \end{array}\right]\)</span>
<ul>
<li>Take matrix to echelon form, then make 0âs above and below all pivots</li>
<li>Normalize each row to make the pivots equal to 1.</li>
<li>Matrix forms I in the pivot rows and columns. The R form above has r pivot columns that make up I and n-r free columns that make up F</li>
</ul></li>
</ul>
<h2 id="elimination-and-factorization">Elimination and Factorization</h2>
<h3 id="gaussian-elimination">Gaussian Elimination</h3>
<ul>
<li>Pick pivot in the first row - pivot must not be zero. If we have a zero pivot, can try to exchange rows to produce a non-zero pivot.</li>
<li>Reduce the numbers below pivot to zero using linear combinations of rows</li>
<li>For well-behaved matrix, will reduce to U, an upper triangular matrix.</li>
<li>Back Substitution
<ul>
<li>Perform elimination on augmented matrix, eg. <span class="math inline">\(\left[\begin{array}{rrr} {2} &amp; {5} \\ {1} &amp; {3} \end{array}\Big|\begin{array}{rrr} {1} \\ {2} \end{array}\right]\)</span></li>
<li>Use upper triangular equations to solve for variables in reverse order <span class="math inline">\(x_1, ...,x_n\)</span>. At each step have one additional unknown in each equation</li>
</ul></li>
<li>Row Echelon Form: Any equation system in row-echelon form always has a âstaircaseâ structure.
<ul>
<li>All rows that contain only zeros are at the bottom of the matrix</li>
<li>Looking at nonzero rows only, the first nonzero number from the left (also called the pivot or the leading coefficient) is always strictly to the right of the pivot of the row above it.</li>
</ul></li>
<li>Reduced Row Echelon Form
<ul>
<li>In row exchlon form</li>
<li>Every pivot is 1</li>
<li>The pivot is the only nonzero entry in its column</li>
</ul></li>
<li>The key idea for finding the solutions of Ax = 0 is to look at the nonpivot columns, which we will need to express as a (linear) combination of the pivot columns. The reduced row echelon form makes this relatively straightforward, and we express the non-pivot columns in terms of sums and multiples of the pivot columns that are on their left</li>
<li>If we bring the augmented equation system into reduced row-echelon form, we can read out the inverse on the right-hand side of the equation system.</li>
<li>Elimination in Matrix Form - all elimination steps could be combined into a single matrix E, that transforms A into U, ie. EA = U</li>
<li>The pivot columns indicate the linearly independent columns - the others are linearly dependent</li>
</ul>
<h3 id="factorization-a-lu">Factorization A= LU</h3>
<ul>
<li>From elimination we have EA = U for some unknown E. For <span class="math inline">\(L = E^{-1}\)</span>, then get A = LU</li>
<li>L adds back to U what E removed from A. If there are no row exchanges, L is just made of the column multipliers</li>
</ul>
<h2 id="vector-spaces">Vector Spaces</h2>
<ul>
<li>All vector spaces contain the origin.</li>
<li>Vectors form a subspace if all linear combinations of those vectors are also in the subspace. Vector space must be closed under linear combinations.</li>
<li>All subspaces of <span class="math inline">\(\R^3\)</span>: <span class="math inline">\(\R^3\)</span>, plane through the origin, line through the origin, zero vector only</li>
<li>Rank of A = # of pivots from elimination</li>
</ul>
<h3 id="groups">Groups</h3>
<ul>
<li>Consider a set G and an operation â : G Ã G â G group defined on G.</li>
<li>Then G := (G, â) is called a group if the following hold:
<ul>
<li>Closure of G under â: âx, y â G : x â y â G</li>
<li>Associativity: âx, y, z â G : (x â y) â z = x â (y â z)</li>
<li>Neutral element: âe â G âx â G : x â e = x and e â x = x 4.</li>
<li>Inverse element: âx â G ây â G : x â y = e and y â x = e. We often write x â1 to denote the inverse element of x</li>
</ul></li>
<li>Vector spaces are groups</li>
</ul>
<h3 id="solving-ax-0">Solving Ax = 0</h3>
<ul>
<li>Elimination does not change solutions, so null space is unchanged by elimination</li>
<li>Can solve Ux = 0 instead then after elimination. Left with pivot columns and free columns, the columns without pivots in which any value can be assigned to the xâs corresponding to that number column</li>
<li>Pivot variables can be found through back substitution, free columns we choose values freely. Set values or free columns to 1 and 0 - this forms our special solution</li>
<li>The null space contains all the combinations of the special solutions. There is one special solution per free variable, and the number of free variables is n - r</li>
</ul>
<h3 id="solving-ax-b">Solving Ax = b</h3>
<ul>
<li>Typical approach - augmented matrix -&gt; elimination</li>
<li>Solvability condition - Ax = b is solvable only when b is in C(A)</li>
<li>Finding complete solution
<ol type="1">
<li>Set all free variables to 0, solve Ax= b for the pivot variables. This gives us a particular solution</li>
<li>Find solutions in the null space</li>
<li>Take linear combination of particular solution and null space solutions. <span class="math inline">\(X_{total} = X_p + X_n\)</span>. <span class="math inline">\(Ax_p = b\)</span> + $Ax_n =0 $ = <span class="math inline">\(A(x_n+x_p) = b\)</span>. With a particular solution, can add anything in the null space and still get b</li>
<li><span class="math inline">\(X_n\)</span> is combination of the special solutions - <span class="math inline">\(X_{total} = X_p + c_1X_{SS1} + c_2X_{SS2}...\)</span></li>
</ol></li>
<li>In short: Find a particular solution to Ax = b, find all solutions to Ax = 0, combine these two results to obtain a general solution</li>
</ul>
<h3 id="rank">Rank</h3>
<ul>
<li># of pivot columns</li>
<li>Dimension of C(A) column space</li>
<li>The column rank equals the row rank</li>
<li>Only full column rank matrices are invertible</li>
</ul>
<h4 id="full-column-rank">Full Column Rank</h4>
<ul>
<li>r = n &lt; m</li>
<li>No free variables, N(A) = {0}</li>
<li>Solution to Ax = b - unique if a solution exists -&gt; 0 or 1 solutions</li>
<li><span class="math inline">\(R = \left[\begin{array}{c} I \\ 0 \end{array}\right]\)</span></li>
</ul>
<h4 id="full-row-rank">Full Row Rank</h4>
<ul>
<li>r = m &lt; n</li>
<li>Solution exists for Ax = b for all b due to free variables</li>
<li>Have n - r = n - m free variables</li>
<li><span class="math inline">\(R = \left[\begin{array}{cc} I &amp; F \end{array}\right]\)</span></li>
</ul>
<h4 id="full-row-column-rank">Full Row + Column Rank</h4>
<ul>
<li>r = m = n -&gt; square matrix, always invertible (defines invertibility)</li>
<li>N(A) = {0}</li>
<li>Ax = b has 1 solution - see this by combing rules for full row and full col rank</li>
<li><span class="math display">\[R = \left[\begin{array}{c} I  \end{array}\right]\]</span></li>
</ul>
<h4 id="singular-matrix-r-m-n">Singular Matrix r &lt; m, n</h4>
<ul>
<li>Ax = b has 0 or <span class="math inline">\(\infty\)</span> solutions</li>
<li><span class="math inline">\(R = \left[\begin{array}{c} I &amp; F \\ 0 &amp; 0\end{array}\right]\)</span></li>
</ul>
<h3 id="independence-span-basis">Independence, Span, Basis</h3>
<ul>
<li><p>Vectors <span class="math inline">\(x_1, ...,x_n\)</span> are independent if no combination gives the zero vector (except the zero combination with scalars = 0): <span class="math inline">\(c_1x_1 + ... + c_nx_n \neq 0\)</span></p></li>
<li><p>Columns are independent if N(A) = {0} <span class="math inline">\(\iff\)</span> rank = n</p></li>
<li><p>Columns are dependent if some vector is in the null space <span class="math inline">\(\iff\)</span> rank &lt; n . Think of 3 vectors in a plane - these must be dependent</p></li>
<li><p>Span - vectors <span class="math inline">\(v_1, ..., v_n\)</span> span a space means a space consists of all combinations of these vectors</p></li>
<li><p>Generating sets are sets of vectors that span vector (sub)spaces, i.e., every vector can be represented as a linear combination of the vectors in the generating set.</p></li>
<li><p>Every linearly independent generating set of V is minimal and is called a basis of V</p></li>
<li><p>Basis - for a vector space, a basis is a sequence of vectors which</p>
<ol type="1">
<li>are Independent</li>
<li>span the space</li>
</ol>
<ul>
<li>A basis is not unique, but all bases for a space have the same number of vectors. In <span class="math inline">\(\R^n\)</span> need n vectors to form basis - this is the dimension D of the space</li>
<li>Finding a basis: write spanning vectors as columns in matrix, reduce to row-echelon form, the spanning vectors associated with the pivot columns form a basis</li>
</ul></li>
<li><p>Dimension D = number of vectors needed to form a basis</p></li>
</ul>
<h3 id="the-four-subspaces">The Four Subspaces</h3>
<h4 id="column-space">Column Space</h4>
<ul>
<li><span class="math inline">\(C(A) \in \R^m\)</span></li>
<li>C(A) = all linear combinations of the columns</li>
<li>We can solve Ax = b when b is in the column space</li>
<li>dim(C(A)) = # of pivot columns = rank(A) = r</li>
<li>Basis = pivot columns</li>
</ul>
<h4 id="null-space">Null Space</h4>
<ul>
<li><span class="math inline">\(N(A) \in \R^n\)</span></li>
<li>All solutions x to equation Ax = 0</li>
<li>Zero vector is always in the null space</li>
<li>The null space contains all the combinations of the special solutions. There is one special solution per free variable, and the number of free variables is n - r</li>
<li>Null space matrix N, RN = 0. <span class="math inline">\(N = \left[\begin{array}{c} -F \\ I \end{array}\right]\)</span></li>
<li>dim(N(A)) = # of free variables = n - r</li>
<li>Basis = special solutions</li>
</ul>
<h4 id="row-space">Row Space</h4>
<ul>
<li>All linear combination of the rows or the column space of <span class="math inline">\(A^T\)</span>, <span class="math inline">\(C(A^T)\)</span></li>
<li><span class="math inline">\(C(A^T) \in \R^n\)</span></li>
<li><span class="math inline">\(dim(C(A^T)) = r\)</span></li>
</ul>
<h4 id="null-transpose-space">Null Transpose Space</h4>
<ul>
<li>Nullspace of <span class="math inline">\(A^T\)</span>, <span class="math inline">\(N(A^T)\)</span>, the left nullspace of A
<ul>
<li>For <span class="math inline">\(A^Ty=0, \, y \in N(A^T) \implies y^TA = 0\)</span></li>
</ul></li>
<li><span class="math inline">\(N(A^T) \in \R^m\)</span></li>
<li><span class="math inline">\(dim(N(A^T)) = m - r\)</span>, the number of free columns in <span class="math inline">\(A^T\)</span></li>
</ul>
<h3 id="matrix-spaces">Matrix Spaces</h3>
<ul>
<li>S = symmetric, U = upper triangular</li>
<li>$S U = $ symmetric + upper triangular = diagonal. S+U is the linear combinations of the matrices in the two spaces</li>
<li><span class="math inline">\(dim(S) + dim(U) = dim(S \cap U) + dim(S+U)\)</span></li>
</ul>
<h3 id="affine-subspace">Affine Subspace</h3>
<ul>
<li>Let V be a vector space, <span class="math inline">\(x_0 \in V , \; U \subset V\)</span> a subspace. Then the subset <span class="math inline">\(\begin{aligned} L &amp;=\boldsymbol{x}_{0}+U:=\left\{\boldsymbol{x}_{0}+\boldsymbol{u}: \boldsymbol{u} \in U\right\} =\left\{\boldsymbol{v} \in V | \exists \boldsymbol{u} \in U: \boldsymbol{v}=\boldsymbol{x}_{0}+\boldsymbol{u}\right\} \subseteq V \end{aligned}\)</span> is called affine subspace or linear manifold of V . U is called direction or direction space, and x0 is called support point.</li>
<li>Examples of affine subspaces are points, lines, and planes in R3 , which do not (necessarily) go through the origin.</li>
</ul>
<h2 id="orthogonality">Orthogonality</h2>
<h3 id="vector-orthogonality">Vector Orthogonality</h3>
<ul>
<li>For two vectors <span class="math inline">\(x \cdot y = x^Ty =0 \iff x \perp y\)</span></li>
<li><span class="math inline">\(x \perp y \implies ||x||^2 + ||y||^2 = ||x+y||^2\)</span></li>
<li>Zero vector orthogonal to all other vectors</li>
</ul>
<h3 id="subspace-orthogonality">Subspace Orthogonality</h3>
<ul>
<li>Subspaces <span class="math inline">\(S \perp T \implies\)</span> all vectors in S perp to all vectors in T</li>
<li><span class="math inline">\(C(A^T) \perp N(A)\)</span>: Row Space orthogonal to Null Space
<ul>
<li>Ax = 0 for x in N(A)</li>
<li>Then by defn x is perpendicular to each row in A using matrix multiplication</li>
<li><span class="math inline">\((c_1row_1 + c_2row_2...)^Tx = 0\)</span></li>
</ul></li>
<li><span class="math inline">\(C(A) \perp N(A^T)\)</span>: Column space orthogonal to left nullspace</li>
<li>Orthogonal complements: a complement contains all vectors perp to the other space
<ul>
<li>Null space and row space are orthogonal complements in <span class="math inline">\(\R^n\)</span></li>
<li>For complements <span class="math inline">\(U, U^T\)</span>, we have <span class="math inline">\(U \cap U^{\perp}=\{\mathbf{0}\}\)</span>. Can decompose a vector in the larger space V as a combination of vectors in the complements: <span class="math inline">\(\boldsymbol{x}=\sum_{m=1}^{M} \lambda_{m} \boldsymbol{b}_{m}+\sum_{j=1}^{D-M} \psi_{j} \boldsymbol{b}_{j}^{\perp}, \quad \lambda_{m}, \psi_{j} \in \mathbb{R}\)</span>.</li>
<li>The orthogonal complement can also be used to describe a plane U (two-dimensional subspace) in a three-dimensional vector space. More specifically, the vector w with <span class="math inline">\(||w|| = 1\)</span>, which is orthogonal to the plane U, is the basis vector of <span class="math inline">\(U^T\)</span></li>
</ul></li>
</ul>
<h3 id="projections">Projections</h3>
<ul>
<li><span class="math inline">\(A^TA\)</span> invertible <span class="math inline">\(\iff\)</span> A has independent columns. Share a rank and null space</li>
<li>p = projection of b onto a. Since p lies on a, it is a scalar multiple of a: <span class="math inline">\(p = xa\)</span>. Left to find x</li>
<li><span class="math inline">\(x = \frac{a^Tb}{a^Ta}\)</span>
<ul>
<li>Define e = b - p = orthogonal vector from b to a vector. Therefore <span class="math inline">\(a \perp b- p\)</span></li>
<li>Then <span class="math inline">\(a^T(b - xa) = 0 \implies xa^Ta = a^Tb \implies x = \frac{a^Tb}{a^Ta}\)</span></li>
</ul></li>
<li><span class="math inline">\(p =xa= a\frac{a^Tb}{a^Ta} = \frac{aa^T}{a^Ta}b = Pb\)</span> for</li>
<li><strong>Projection Matrix</strong> <span class="math inline">\(P = \frac{aa^T}{a^Ta}\)</span>
<ul>
<li><span class="math inline">\(P^T = P\)</span> - symmetric</li>
<li><span class="math inline">\(P^2 = P\)</span> - projection of projection same as single projection</li>
</ul></li>
<li>Point of projection - Ax = b may have no solutions, ie. b not in C(A). Can instead solve <span class="math inline">\(A\hat{x} = p\)</span>, where p is the projection of b onto the column space and <span class="math inline">\(\hat{x}\)</span> is the solution to this altered problem.</li>
<li>Higher dimensions - <span class="math inline">\(p = A\hat{x}\)</span>
<ul>
<li>Key is <span class="math inline">\(e = b - A\hat{x} \perp plane\)</span></li>
<li><strong><span class="math inline">\(A^TA\hat{x} = A^Tb\)</span></strong></li>
<li><strong><span class="math inline">\(\hat{x} = (A^TA)^{-1}A^Tb\)</span></strong></li>
<li>projection matrix <span class="math inline">\(P = A(A^TA)^{-1}A^T\)</span>. The inverse cannot be distributed bc A not necessarily square - <span class="math inline">\(A^TA\)</span> is square however</li>
</ul></li>
<li>Some implications:
<ul>
<li>If b in C(A), <span class="math inline">\(Pb = b\)</span>. Derived from <span class="math inline">\(b = Ax \implies A(A^TA)^{-1}A^TAx = Ax = b\)</span></li>
<li>If <span class="math inline">\(b \perp C(A), \, Pb =0\)</span>. Derived from <span class="math inline">\(A(A^TA)^{-1}A^Tb = A(A^TA)^{-1}(0) = 0\)</span></li>
</ul></li>
</ul>
<h3 id="least-squares">Least Squares</h3>
<ul>
<li>Given number of non-collinear points, have some line <span class="math inline">\(Ax = b\)</span> with errors <span class="math inline">\(||e||^2 = e^2_1 + e_2^2 + ...\)</span></li>
<li>We take points on the line <span class="math inline">\(p_1, p_2, p_3, ...\)</span> instead of original points <span class="math inline">\(b_1, b_2, b_3,...\)</span></li>
<li>Use <span class="math inline">\(A^TA\hat{x} = A^Tb\)</span> to derive normal equations. The partial derivatives of <span class="math inline">\(||Ax- b||^2\)</span> are zero when <span class="math inline">\(A^TA\hat{x} = A^Tb\)</span></li>
<li>If A has independent columns, then <span class="math inline">\(A^TA\)</span> is invertible is crucial to making this work.
<ul>
<li>If <span class="math inline">\(A^TAx = 0\)</span>, then x can only be the zero vector</li>
</ul></li>
</ul>
<h3 id="orthogonal-matrices">Orthogonal Matrices</h3>
<ul>
<li>Orthonormal vectors: <span class="math inline">\(q^T_iq_j = \begin{cases} 0 &amp; if \; i = j \\ 1 &amp; if \; i \neq j\end{cases}\)</span></li>
<li><span class="math inline">\(Q = \left[\begin{array}{c} q_1 q_2...q_n\end{array}\right]\)</span></li>
<li><span class="math inline">\(Q^TQ = I\)</span>. For square Q, this implies <span class="math inline">\(Q^T = Q^{-1}\)</span></li>
<li>Q has orthonormal columns, to project onto its column space <span class="math inline">\(P = Q(Q^TQ)^{-1}Q^T = QQ^T\)</span>
<ul>
<li>If P is square, then projection is onto whole space <span class="math inline">\(QQ^T = I\)</span></li>
</ul></li>
<li>Transformations by orthogonal matrices are special because the length of a vector x is not changed when transforming it using an orthogonal matrix A.</li>
</ul>
<h3 id="orthonormal-basis">Orthonormal Basis</h3>
<ul>
<li>The basis vectors are orthogonal to each other and where the length of each basis vector is 1.</li>
<li><span class="math inline">\(\left\langle\boldsymbol{b}_{i}, \boldsymbol{b}_{j}\right\rangle= 0 \quad \text { for } i \neq j\)</span> and <span class="math inline">\(\left\langle\boldsymbol{b}_{i}, \boldsymbol{b}_{i}\right\rangle= 1\)</span></li>
<li>Gram-Schmidt is the process for forming an orthonormal basis.</li>
</ul>
<h3 id="gram-schmidt">Gram-Schmidt</h3>
<ul>
<li>Process to find orthonormal projection - method iteratively constructs and orthogonal basis from any basis of V
<ul>
<li>High-dimensional data quite often possesses the property that only a few dimensions contain most information, and most other dimensions are not essential to describe key properties of the data. Compression causes loss of information, so we need to find the most informative dimensions in the data</li>
<li>The idea is to find the vector in the subspace spanned by the columns of A that is closest to b, i.e., we compute the orthogonal projection of b onto the subspace spanned by the columns of A -&gt; the least-squares solution</li>
</ul></li>
<li>For independent vectors a, b, c, let A, B, C be orthogonal, then <span class="math inline">\(q_1 = \frac{A}{||A||}, \, q_2 = \frac{B}{||B||}, q_3 = \frac{C}{||C||}\)</span></li>
<li>Let a = A, then need to change b to be orthogonal to a. Requires B = e, the error vector</li>
<li><span class="math inline">\(B = b - \frac{A^Tb}{A^TA}A\)</span>, then <span class="math inline">\(A^TB = 0 \implies A\perp B\)</span></li>
<li>To make C, need a third vector orthogonal to both A, B by subtracting off the components in the a, b directions</li>
<li><span class="math inline">\(C = c - \frac{A^Tc}{A^TA}A - \frac{B^Tc}{B^TB}B\)</span></li>
<li>A = QR
<ul>
<li>Basic expression of G-S</li>
<li><span class="math inline">\(A = \left[\begin{array}{c} a_1 a_2\end{array}\right] = \left[\begin{array}{c} q_1 q_2\end{array}\right]R\)</span> for R upper triangular</li>
</ul></li>
</ul>
<h3 id="projection-onto-affine-subspaces">Projection onto Affine Subspaces</h3>
<ul>
<li>Given affine space <span class="math inline">\(L = x_0 + U\)</span> with <span class="math inline">\(b_1, b_2\)</span> basis vectors for U</li>
<li>Transform <span class="math inline">\(L-x_{0}=U\)</span>, now can use projection onto a vector subspace. Projection equals <span class="math inline">\(\pi_{L}(\boldsymbol{x})=\boldsymbol{x}_{0}+\pi_{U}\left(\boldsymbol{x}-\boldsymbol{x}_{0}\right)\)</span></li>
</ul>
<h2 id="determinants">Determinants</h2>
<ul>
<li>A number associated with every <strong>square</strong> matrix. Test for invertibility when <span class="math inline">\(det(A) \neq 0\)</span></li>
<li>The determinant det(A) is the signed volume of an n-dimensional parallelepiped formed by columns of the matrix A.</li>
<li>If A invertible, <span class="math inline">\(\operatorname{det}\left(\boldsymbol{A}^{-1}\right)=\frac{1}{\operatorname{det}(\boldsymbol{A})}\)</span></li>
<li>Similar matrices possess the same determinant - for a linear mapping all transformation matrices have the same determinant. Therefore the determinant is invariant to the choice of basis of a linear mapping.</li>
<li>Characteristic polynomial for square matrix A: <span class="math inline">\(p_{A}(\lambda):=\operatorname{det}(A-\lambda I) = =c_{0}+c_{1} \lambda+c_{2} \lambda^{2}+\cdots+c_{n-1} \lambda^{n-1}+(-1)^{n} \lambda^{n}\)</span>. Note $ c_{0} =() ,; c_{n-1} =(-1)^{n-1} ()$</li>
</ul>
<h3 id="properties">Properties</h3>
<ol type="1">
<li><p><span class="math inline">\(det\; I = 1\)</span></p></li>
<li><p>Row exchanges: for each row exchange, reverse the sign of the determinant</p>
<ul>
<li>Det of permutations either +- 1 depending on if we do even or odd number of exchanges</li>
</ul></li>
<li><p>Scalar factoring and linear function</p>
<ol type="a">
<li><p>Scalar factor can be pulled out of a row: <span class="math inline">\(\left[\begin{array}{c} ta &amp; tb \\ c &amp; d\end{array}\right] = t \left[\begin{array}{c} a &amp; b \\ c &amp; d\end{array}\right]\)</span></p></li>
<li><p>Determinant is a linear function of rows (within row, not globally <span class="math inline">\(det(A+B) \neq det(A) + det(B)\)</span>): <span class="math inline">\(\left[\begin{array}{c} a + a&#39; &amp; b + b&#39; \\ c &amp; d\end{array}\right] = \left[\begin{array}{c} a &amp; b \\ c &amp; d\end{array}\right] + \left[\begin{array}{c} a&#39; &amp; b&#39; \\ c &amp; d\end{array}\right]\)</span></p></li>
</ol></li>
<li><p>2 equal rows <span class="math inline">\(\implies det = 0\)</span></p>
<ul>
<li>Proof: exchange the equal rows, the determinant should be the same since matrix is unchanged but violates property 2. Therefore must be 0</li>
</ul></li>
<li><p>Subtraction scaled row l from row k <span class="math inline">\(\implies\)</span> determinant does not change, ie. elimination does not change determinant</p>
<ul>
<li><span class="math inline">\(\left[\begin{array}{c} a &amp; b \\ c - la &amp; d - lb\end{array}\right] = \left[\begin{array}{c} a &amp; b \\ c &amp; d\end{array}\right] + \left[\begin{array}{c} a &amp; b \\ -la &amp; -lb\end{array}\right] = \left[\begin{array}{c} a &amp; b \\ c &amp; d\end{array}\right] + -l\left[\begin{array}{c} a &amp; b \\ a &amp; b\end{array}\right] = \left[\begin{array}{c} a &amp; b \\ c &amp; d\end{array}\right]\)</span></li>
<li>Proved using properties 3b, 3a, and 4</li>
</ul></li>
<li><p>Row of 0âs <span class="math inline">\(\implies det = 0\)</span></p>
<ul>
<li>ie. elimination gives zero row - singular and non-invertible</li>
<li>Say t = 0, then det = 0 by 3b</li>
</ul></li>
<li><p>Product of diagonals for triagular matrix: <span class="math inline">\(det \; U = d_1 \times d_2 \times ... \times d_n\)</span></p>
<ul>
<li>Product of pivots after elimination (with sign determined by row exchanges too)</li>
<li>Using properties 5, 3a, 1 could make diagonal and factor out dâs: <span class="math inline">\(d_1...d_n(I)\)</span></li>
</ul></li>
<li><p><span class="math inline">\(det \: A =0 \iff\)</span> A is singular</p>
<ul>
<li>Another way of seeing A is invertible only when have pivots of full rank</li>
</ul></li>
<li><p><span class="math inline">\(det\;AB = (det \; A)(det \; B)\)</span></p></li>
<li><p><span class="math inline">\(det\; A^T = det\;A\)</span></p></li>
</ol>
<h3 id="cofactors">Cofactors</h3>
<ul>
<li><span class="math inline">\(det \; A = \sum_{n! \;terms} \pm a_{1\alpha}a_{2\beta}a_{3\gamma}...a_{n\omega}\)</span></li>
<li>For 3x3: <span class="math inline">\(\left|\begin{array}{ccc} a_{11} &amp; a_{12} &amp; a_{13} \\ a_{21} &amp; a_{22} &amp; a_{23} \\ a_{31} &amp; a_{32} &amp; a_{33}\end{array}\right| = a_{11}a_{22}a_{33} + (-1)a_{11}a_{23}a_{32} + a_{12}a_{21}a_{33} + (-1) a_{12}a_{23}a_{31} + a_{13}a_{21}a_{32} + (-1)a_{13}a_{22}a_{31}\)</span></li>
<li>Basic approach: determinant of smaller sub matrix: <span class="math inline">\(\left|\begin{array}{ccc} a_{11} &amp; &amp; \\ &amp; a_{22} &amp; a_{23} \\ &amp; a_{32} &amp; a_{33}\end{array}\right|\)</span></li>
<li>Cofactor of <span class="math inline">\(a_{ij} = C_{ij}\)</span>, given + if i + j is even, - if i+j odd: <span class="math inline">\(\left|\begin{array}{ccc} + &amp; - &amp; + \\ - &amp; + &amp; - \\ + &amp; - &amp; +\end{array}\right|\)</span></li>
<li>Cofactor formula: <span class="math inline">\(det \; A = a_{11}C_{11} + ... + a_{1n}C_{1n}\)</span> along a single row (here row 1)</li>
</ul>
<h2 id="eigenvalues-and-eigenvectors">Eigenvalues and Eigenvectors</h2>
<ul>
<li><p><span class="math inline">\(Ax = \lambda x\)</span> - for a square matrix A</p>
<ul>
<li>eigenvector - a vector that fits the property Ax || x</li>
<li>eigenvalue - some scalar value that allows eigenvector property to hold</li>
</ul></li>
<li><p>All vectors that are collinear to x are also eigenvectors of A.</p></li>
<li><p>Eigenspace - the set of all eigenvectors of A associated with an eigenvalue <span class="math inline">\(\lambda\)</span> spans a subspace, the eigenspace of A wrt <span class="math inline">\(\lambda\)</span>, denoted <span class="math inline">\(E_\lambda\)</span>. The set of all eigenvalues of A is called the eigenspectrum (or spectrum) of A. The eigenspace for <span class="math inline">\(\lambda\)</span> solves the system <span class="math inline">\((A - \lambda I)x = 0\)</span>, ie. its the null space of the <span class="math inline">\((A - \lambda I)\)</span></p></li>
<li><p>If A is singular, 0 will be an eigenvalue</p></li>
<li><p>Projection matrix - Px = x with <span class="math inline">\(\lambda = 1\)</span> for vectors in the plane, <span class="math inline">\(\lambda = 0\)</span> for x fitting Px = 0</p></li>
<li><p>Permutation matrix - <span class="math inline">\(\lambda = 1, -1\)</span></p></li>
<li><p>Trace = sum down diagonal of A = <span class="math inline">\(a_{11} + a_{22} + ... + a_{nn}\)</span></p></li>
<li><p>The identity matrix has a single eigenvalue 1 repeated n times and clearly the eigenspace spans the full n dimensions.</p></li>
<li><p>Geometrically, the eigenvector corresponding to a nonzero eigenvalue points in a direction that is stretched by the linear mapping. The eigenvalue is the factor by which it is stretched. If the eigenvalue is negative, the direction of the stretching is flipped.</p></li>
<li><p>It is often a convention that eigenvalues are sorted in descending order, so that the largest eigenvalue and associated eigenvector are called the first eigenvalue and its associated eigenvector, and the second largest called the second eigenvalue and its associated eigenvector, and so on.</p></li>
</ul>
<h3 id="tricks">Tricks</h3>
<ul>
<li>With symmetric matrices, eigenvalues will always be real</li>
<li>Eigenvalues are always complex conjugates</li>
<li>Triangular matrix - eigenvalues are just the diagonal values</li>
<li>A matrix and its transpose possess the same eigenvalues (not necessarily same eigenvectors)</li>
<li>Similar matrices have the same eigenvalues - eigenvalues are independent of the basis for a transformation matrix.</li>
</ul>
<h3 id="general-procedure">General Procedure</h3>
<ul>
<li>Write <span class="math inline">\(Ax = \lambda x\)</span></li>
<li>Rewrite as <span class="math inline">\((A - \lambda I)x = 0\)</span>, giving us a singular matrix <span class="math inline">\((A - \lambda I)\)</span> with det = 0</li>
<li>Get characteristic equation (polynomial) from <span class="math inline">\(det(A - \lambda I) = 0\)</span></li>
<li>Solve for <span class="math inline">\(\lambda\)</span> as root to characteristic equation</li>
<li>An n x n matrix will have n eigenvalues, though could be repeated</li>
<li>Once lambdas found, find x with elimination, finding the null space of the singular matrix with lambda values plugged in</li>
<li>If repeated eigenvalue, will have fewer eigenvectors. Otherwise the n eigenvectors for n eigenvalues are linearly independent.</li>
<li><strong>Eigenvalues sum to trace, multiply to determinant</strong>. Put another way the determinant is the product of eigenvalues <span class="math inline">\(\operatorname{det}(\boldsymbol{A})=\prod_{i=1}^{n} \lambda_{i}\)</span> and the trace is the sum of the eigenvalues <span class="math inline">\(\operatorname{tr}(\boldsymbol{A})=\sum_{i=1}^{n} \lambda_{i}\)</span></li>
</ul>
<h3 id="diagonalization-eigendecomposition">Diagonalization / Eigendecomposition</h3>
<ul>
<li>Suppose n linearly independent eigenvectors of A - form columns of matrix S</li>
<li><span class="math inline">\(AS = A\left[\begin{array}{c}x_1x_2...x_n\end{array}\right] = \left[\begin{array}{c}\lambda_1x_1...\lambda_nx_n\end{array}\right] = \left[\begin{array}{c}x_1x_2...x_n\end{array}\right]\left[\begin{array}{c}\lambda_1 &amp; 0 &amp; ... &amp; 0\\ 0&amp; \lambda_2 &amp; ... &amp; 0 \\ 0&amp; 0 &amp; ... &amp; \lambda_n\end{array}\right] = S\Lambda\)</span></li>
<li>Key: $ A = SS^{-1}$ - A is similar to a diagonal matrix of eigenvalues</li>
<li>Powers of A: <span class="math inline">\(A^K = S \Lambda^KS^{-1}\)</span></li>
<li>Theorem: <span class="math inline">\(A^K \rightarrow 0 \text{ as } k \rightarrow \infty\)</span> if all <span class="math inline">\(|\lambda_i| &lt; 1\)</span>
<ul>
<li>dependent on assumption of n independent eigenvectors, otherwise cannot diagonalize</li>
</ul></li>
<li>A is sure to be diagonalizable if all eigenvalues are different. If eigenvalues repeated, may or may not have n independent eigenvectors</li>
<li>Can use to solve recursive formulae: <span class="math inline">\(u_{k+1} = Au_k\)</span> then can solve using <span class="math inline">\(u_{k+1} = A^{k+1}u_0\)</span></li>
<li>Procedure
<ul>
<li>Compute eigenvalues and eigenvectors</li>
<li>Check that A can be diagonalized - do the eigenvectors form a basis in <span class="math inline">\(\R^n\)</span></li>
<li>Construct the transformation matrix P - collect eigenvectors of A in <span class="math inline">\(S:= \left[x_1\;x_2\;...\right]\)</span></li>
</ul></li>
</ul>
<h3 id="symmetric-matrices">Symmetric Matrices</h3>
<ul>
<li>Eigenvalues are always real and eigenvectors can be chosen to be orthogonal</li>
<li>While usually have <span class="math inline">\(A = S\Lambda S^{-1}\)</span>, now have <span class="math inline">\(A = Q\Lambda Q^{-1}= Q\Lambda Q^{T}\)</span> (can make the eigenvectors orthonormal to fit Q defn)</li>
<li>Notation for complex conjugates: x is conjugate of <span class="math inline">\(\bar{x}\)</span></li>
<li>Defn symmetric <span class="math inline">\(A = A^T\)</span> for real, otherwise <span class="math inline">\(A = \bar{A}^T\)</span>. <span class="math inline">\(A = Q\Lambda Q^T = \left[\begin{array}{c}q_1...q_n\end{array}\right]\left[\begin{array}{c}\lambda_1 &amp; 0 &amp; ... &amp; 0\\ 0&amp; \lambda_2 &amp; ... &amp; 0 \\ 0&amp; 0 &amp; ... &amp; \lambda_n\end{array}\right] \left[\begin{array}{c}q_1\\...\\q_n\end{array}\right] = \lambda_1q_1q_1^T + ... \lambda_nq_nq_n^T\)</span></li>
<li>Each <span class="math inline">\(qq^T\)</span> is a projection matrix - every symmetric matrix is a combination of perpendicular projection matrices</li>
<li>For symmetric matrices, signs of the pivots are the same as the signs of the eigenvalues: # pos pivots = # pos eigenvalues</li>
</ul>
<h3 id="positive-definite-matrices">Positive Definite Matrices</h3>
<ul>
<li>Symmetric matrices with only positive eigenvalues</li>
<li>All pivots are positive, all subdeterminants are positive</li>
<li>Tests
<ol type="1">
<li>Eigenvalues: <span class="math inline">\(\lambda &gt; 0 \;\forall \lambda\)</span></li>
<li>Determinant: <span class="math inline">\(a &gt; 0, \; ac - b^2 &gt; 0\)</span></li>
<li>Pivots: <span class="math inline">\(a &gt; 0,\; \frac{ac-b^2}{a} &gt; 0\)</span></li>
<li>Key Test: <span class="math inline">\(x^TAx &gt; 0\)</span></li>
</ol></li>
<li>If only <span class="math inline">\(x^TAx \geq 0\)</span> holds then the matrix is positive semi-definite</li>
<li><span class="math inline">\(x^TAx\)</span> produces a quadratic form: <span class="math inline">\(A = \left[\begin{array}{c} 2 &amp; 6 \\ 6 &amp; 20\end{array}\right] \implies x^TAx = \left[\begin{array}{c}x_1&amp;x_2\end{array}\right]\left[\begin{array}{c}2x_1 + 6x_2 \\6x_1 + 20x_2\end{array}\right] = 2x_1^2 + 12x_1x_2 + 20x^2_2\)</span></li>
<li>det = 4, trace = 22 - then both eigenvalues must be positive. <span class="math inline">\(x^TAx &gt; 0\)</span> except at x = 0</li>
<li>Intuition is we need the squares to overwhelm the combined term. Notice <span class="math inline">\(a_{11},\;a_{22}\)</span> are the cofficients for the squares</li>
<li>Essentially minimizing <span class="math inline">\(f(x,y) = 2x^2 + 12xy + 20y^2 = 2(x+3y)^2 + 2y^2\)</span> - factoring using complete the square shows the sum of two squares -&gt; always positive</li>
<li>If A,B pos def, then A + B also pos def. Notice for least squares, <span class="math inline">\(A^TA\)</span> is square and symmetric, so <span class="math inline">\(x^TA^TAx = (Ax)^T(Ax) = ||Ax||^2 \geq 0\)</span>. Only 0 when the vector is 0, so for any matrix of rank n can say least squares will be positive definite</li>
<li>For pos, def A <span class="math inline">\(\langle\boldsymbol{x}, \boldsymbol{y}\rangle=\hat{\boldsymbol{x}}^{\top} \boldsymbol{A} \hat{\boldsymbol{y}}\)</span> defines an inner product with respect to basis B where <span class="math inline">\(\tilde{\boldsymbol{x}}, \tilde{\boldsymbol{y}}\)</span> are the coordinate representations of x,y in the basis B</li>
<li>The null space of A consists only of zero vector because <span class="math inline">\(x^{\top} A x&gt;0\)</span> for all <span class="math inline">\(x \neq 0\)</span></li>
<li>We can always produce a positive, semidefinite matrix by <span class="math inline">\(\boldsymbol{S}:=\boldsymbol{A}^{\top} \boldsymbol{A}\)</span>. If A has full column rank then positive definite.</li>
</ul>
<h3 id="complex-matrices">Complex Matrices</h3>
<ul>
<li>For complex <span class="math inline">\(z = \left[\begin{array}{c}z_1\\...\\z_n\end{array}\right] \in \C^n\)</span>, cannot use <span class="math inline">\(z^Tz\)</span> for length squared since it is negative!</li>
<li>Instead use <span class="math inline">\(\bar{z}^Tz = ||z||^2\)</span>, where z-bar is the complex conjugate (flipped sign on complex part). Eg. <span class="math inline">\(z = \left[\begin{array}{c}1\\i\end{array}\right], \; \bar{z} = \left[\begin{array}{c}1\\-i\end{array}\right]\)</span></li>
<li>Hermetian: <span class="math inline">\(z^Hz = \bar{z}^Tz\)</span> - use this for the inner product when dealing with complex vectors</li>
<li>Hermetian matrices: <span class="math inline">\(A^H = A\)</span> - real eigenvalues, orthogonal eigenvectors. Just like real symmetric matrices</li>
</ul>
<h3 id="similar-matrices">Similar Matrices</h3>
<ul>
<li>For 2 square matrices, A and B are similar if they have the same eigenvalues (easily checked with trace / determinant)</li>
<li>For some matrix, can factor B: <span class="math inline">\(B = M^{-1}AM\)</span></li>
<li>Have already seen a similar matrix in <span class="math inline">\(\Lambda = S^{-1}AS \implies\)</span> A is similar to <span class="math inline">\(\Lambda\)</span></li>
<li>All matrices with same eigenvalues of A are similar to A and can be transformed via some M - form families</li>
<li>If <span class="math inline">\(\lambda_1 = \lambda_2\)</span>, then might not be diagonalizable depending if there is one eigenvector or two - this will split into different families.</li>
</ul>
<h4 id="jordan-form">Jordan Form</h4>
<ul>
<li>Take the most diagonalizable family of similar matrices. Not always easy to do in practice since we need exactly the same eigenvalues</li>
<li>Create jordon blocks that contain a single eigvector: <span class="math inline">\(J_i = \left[\begin{array}{c}\lambda_i &amp; 0 &amp; ... &amp; 0\\ 0&amp; \lambda_i &amp; ... &amp; 0 \\ 0&amp; 0 &amp; ... &amp; \lambda_i\end{array}\right]\)</span></li>
<li>Every square matrix A is similar to a jordan matrix made of these jordon blocks.</li>
<li># blocks = # of eigenvectors</li>
</ul>
<h2 id="decompositions">Decompositions</h2>
<h3 id="cholesky-decomposition">Cholesky Decomposition</h3>
<ul>
<li>A square root equivalence on symmetric, positive definite matrices</li>
<li><span class="math inline">\(A=L L^{T}\)</span> where L is a lower triangular matrix with positive diagonal elements: <span class="math inline">\(\left[\begin{array}{ccc} {a_{11}} &amp; {\cdots} &amp; {a_{1 n}} \\ {\vdots} &amp; {\ddots} &amp; {\vdots} \\ {a_{n 1}} &amp; {\cdots} &amp; {a_{n n}} \end{array}\right]=\left[\begin{array}{ccc} {l_{11}} &amp; {\cdots} &amp; {0} \\ {\vdots} &amp; {\ddots} &amp; {\vdots} \\ {l_{n 1}} &amp; {\cdots} &amp; {l_{n n}} \end{array}\right]\left[\begin{array}{ccc} {l_{11}} &amp; {\cdots} &amp; {l_{n 1}} \\ {\vdots} &amp; {\ddots} &amp; {\vdots} \\ {0} &amp; {\cdots} &amp; {l_{n n}} \end{array}\right]\)</span></li>
<li>Eg <span class="math inline">\(\boldsymbol{A}=\left[\begin{array}{ccc} {l_{11}^{2}} &amp; {l_{21} l_{11}} &amp; {l_{31} l_{11}} \\ {l_{21} l_{11}} &amp; {l_{21}^{2}+l_{22}^{2}} &amp; {l_{31} l_{21}+l_{32} l_{22}} \\ {l_{31} l_{11}} &amp; {l_{31} l_{21}+l_{32} l_{22}} &amp; {l_{31}^{2}+l_{32}^{2}+l_{33}^{2}} \end{array}\right]\)</span>, L is the Cholesky factor of A and L is unique.</li>
<li>We can backward calculate what the components <span class="math inline">\(l_{ij}\)</span> should be given the values for A and previously computed elements of L.</li>
<li>The covariance matrix of a multivariate Gaussian variable (see Section 6.5) is symmetric, positive definite. The Cholesky factorization of this covariance matrix allows us to generate samples from a Gaussian distribution. It also allows us to perform a linear transformation of random variables, which is heavily exploited when computing gradients in deep stochastic models.</li>
<li>Given the Cholesky decomposition <span class="math inline">\(A = LL^T\)</span> , we know that <span class="math inline">\(det(A) = det(L) det(L^T) = det(L)^2\)</span> . Since L is a triangular matrix, the determinant is simply the product of its diagonal entries so that <span class="math inline">\(det(A) = \prod_{i} l_{i i}^{2}\)</span>.</li>
</ul>
<h3 id="singular-value-decomposition">Singular Value Decomposition</h3>
<ul>
<li><span class="math inline">\(A = U\Sigma V^T\)</span> for <span class="math inline">\(\Sigma\)</span> diagonal, U,V orthogonal. Exists for any matrix A.</li>
<li>Special case of positive definite: <span class="math inline">\(A = Q\Lambda Q^T\)</span></li>
<li>Basic Idea: for V a basis in the row space, U a basis in the col space, and for sigma scaling factors:
<ul>
<li><span class="math inline">\(AV = U\Sigma \implies A\left[\begin{array}{c}v_1...v_r\end{array}\right]= \left[\begin{array}{c}u_1...u_r\end{array}\right]\left[\begin{array}{c}\sigma_1 &amp; &amp;\\ &amp; \sigma_2 &amp; \\&amp; &amp;...\end{array}\right]\)</span></li>
<li><span class="math inline">\(A = U\Sigma V^{-1} = U\Sigma V^T\)</span></li>
</ul></li>
<li>The matrix S consists of the singular values. Ordered largest in the top left to smallest in the bottom right. S is unique, m x n rectangular - it is the same size as A. If m &gt; n, square matrix of sigmas on top of matrix of 0. For n &gt; m, square matrix of sigmas to the left of a matrix of 0âs.</li>
<li>The SVD expresses a change of basis in both the domain and codomain. This is in contrast with the eigendecomposition that operates within the same vector space, where the same basis change is applied and then undone. What makes the SVD special is that these two different bases are simultaneously linked by the singular value matrix S.</li>
<li>The left singular vectors of A are eigenvectors of <span class="math inline">\(AA^T\)</span>. The right singular vectors of A are the eigenvectors of <span class="math inline">\(A^TA\)</span>. The non zero singular values of A are the square roots of the nonzero eigenvalues of <span class="math inline">\(AA^T\)</span> and are equal to the nonzero eigenvalues of <span class="math inline">\(A^TA\)</span>.</li>
<li>Substituting a matrix with its SVD has often the advantage of making calculation more robust to numerical rounding errors.</li>
</ul>
<h5 id="interpretations">Interpretations</h5>
<ul>
<li>Expresses every row of A as a linear combination of the rows of <span class="math inline">\(V^T\)</span>, the right singular vectors. The rows of US are the coefficients to those combinations</li>
<li>Expresses every column of A as linear combination of the columns of U, the left singular vectors, with coefficients given by <span class="math inline">\(SV^T\)</span>. Therefore we interpret just the rows or columns of the decomposition, we can say something important about A.</li>
<li>Say rows are customers, columns products, values ratings. The right singular values could be customer types, with each customer defined as a linear mixture of types. Left sigular values could be product types and SVD expresses A as a mixture of product types.</li>
<li>When only a single direction is interesting, can use PCA instead of a full SVD.</li>
</ul>
<h5 id="procedure">Procedure</h5>
<ul>
<li>Using <span class="math inline">\(A = \left[\begin{array}{cc} 4 &amp; 4 \\ -3 &amp; 3\end{array}\right]\)</span></li>
</ul>
<ol type="1">
<li><span class="math inline">\(A^TA = \left[\begin{array}{cc} 4 &amp; -3 \\ 4 &amp; 3\end{array}\right]\left[\begin{array}{cc} 4 &amp; 4 \\ -3 &amp; 3\end{array}\right] = \left[\begin{array}{cc} 25 &amp; 7 \\ 7 &amp; 25\end{array}\right]\)</span></li>
<li>Find eigens of <span class="math inline">\(A^TA\)</span>: <span class="math inline">\(x_1 = \left[\begin{array}{cc} 1 \\ 1\end{array}\right], \; x_2 = \left[\begin{array}{cc} 1 \\ -1\end{array}\right]\)</span> and <span class="math inline">\(\lambda_1 = 32, \; \lambda_2 = 18\)</span>. Normalize eigenvectors: divide by length (here <span class="math inline">\(\sqrt{2}\)</span>)</li>
<li>Set up: <span class="math inline">\(A = \left[\begin{array}{cc} 4 &amp; 4 \\ -3 &amp; 3\end{array}\right] = A = \left[\begin{array}{cc} &amp; \\ &amp; \end{array}\right]\left[\begin{array}{cc} \sqrt{32} &amp; 0 \\ 0 &amp; \sqrt{18}\end{array}\right]\left[\begin{array}{cc} 1/\sqrt{2} &amp; 1/\sqrt{2} \\ 1/\sqrt{2} &amp; -1/\sqrt{2}\end{array}\right] = U\Sigma V^T\)</span></li>
<li>Find Uâs. <span class="math inline">\(AA^T\)</span> is a positive definite symmetric matrix. <span class="math inline">\(AA^T = U\Sigma V^T V \Sigma^T U^T = U\Sigma\Sigma^T U^T\)</span>
<ul>
<li>Calc <span class="math inline">\(AA^T\)</span>: <span class="math inline">\(AA^T = \left[\begin{array}{cc} 4 &amp; 4 \\ -3 &amp; 3\end{array}\right]\left[\begin{array}{cc} 4 &amp; -3 \\ 4 &amp; 3\end{array}\right] = \left[\begin{array}{cc} 32 &amp; 0 \\ 0 &amp; 18\end{array}\right]\)</span></li>
<li><span class="math inline">\(\lambdaâs\)</span> of <span class="math inline">\(AA^T\)</span> are the same as for <span class="math inline">\(A^TA\)</span> - (32, 18). <span class="math inline">\(x_1 = \left[\begin{array}{cc} 1 \\ 0\end{array}\right], \; x_2 = \left[\begin{array}{cc} 0 \\ 1\end{array}\right]\)</span></li>
<li>Then <span class="math inline">\(U = x_1 = \left[\begin{array}{cc} x_1 &amp; x_2\end{array}\right] = \left[\begin{array}{cc} 1 &amp; 0 \\ 0 &amp; 1\end{array}\right]\)</span></li>
</ul></li>
<li>$A = UV^T = $</li>
</ol>
<h2 id="linear-transformations">Linear Transformations</h2>
<ul>
<li>Follow two rules
<ol type="1">
<li><span class="math inline">\(T(v+w) = T(v) + T(w)\)</span></li>
<li><span class="math inline">\(T(cV) = cT(v)\)</span></li>
</ol></li>
<li>If we want to use a matrix, we need to use coordinates that the transformation is relative to: <span class="math inline">\(T(v) = Av\)</span></li>
<li>Coordinates come from a basis - <span class="math inline">\(v = c_1v_1 + ... + c_nv_n\)</span>. We typically assume the standard basis but not necessary</li>
<li>Definitions for Transformations <span class="math inline">\(\Phi\)</span> of Two Vector Spaces V, W
<ul>
<li>Injective if <span class="math inline">\(\Phi(x) = \Phi(y) \implies x = y\)</span></li>
<li>Surjective if <span class="math inline">\(\Phi(V) = W\)</span></li>
<li>Bijective if injective and surjective. Every element in W can be âreachedâ via the mapping from V. A reversible mapping must also exist <span class="math inline">\(\Psi = \Phi^{-1}\)</span></li>
</ul></li>
<li>Special Transformations
<ul>
<li>Isomorphism: V to W linear and bijective
<ul>
<li>Finite-dimensional vector spaces V and W are isomorphic if and only if dim(V ) = dim(W). Intuitively, this means that vector spaces of the same dimension are kind of the same thing, as they can be transformed into each other without incurring any loss.</li>
</ul></li>
<li>Endomorphism: V to V linear</li>
<li>Automorphism: V to V linear and bijective</li>
</ul></li>
</ul>
<h3 id="matrix-form">Matrix Form</h3>
<ul>
<li>Consider a vector space V and an ordered basis B = (b1, . . . , bn) of V . For any x in V we obtain a unique representation (linear combination) <span class="math inline">\(\boldsymbol{x}=\alpha_{1} \boldsymbol{b}_{1}+\ldots+\alpha_{n} \boldsymbol{b}_{n}\)</span> of x with respect to B. Then a1, . . . , an are the coordinates of x with respect to B, and the vector <span class="math inline">\(\boldsymbol{\alpha}=\left[\begin{array}{c} {\alpha_{1}} \\ {\vdots} \\ {\alpha_{n}} \end{array}\right]\)</span> is the coordinate vector/coordinate representation of x with respect to the ordered basis B.</li>
<li>For vector spaces V, W with ordered bases B in Rn, C in Rm, take linear mapping <span class="math inline">\(\Phi: V \rightarrow W\)</span>, then we can represent B uniquely in terms of C as <span class="math inline">\(\Phi\left(\boldsymbol{b}_{j}\right)=\alpha_{1 j} \boldsymbol{c}_{1}+\cdots+\alpha_{m j} \boldsymbol{c}_{m}=\sum_{i=1}^{m} \alpha_{i j} \boldsymbol{c}_{i}\)</span>. The transformation matrix is then defined by m x n A: <span class="math inline">\(A_{\Phi}(i, j)=\alpha_{i j}\)</span>.</li>
<li>The transformation matrix can be used to map coordinates with respect to an ordered basis in V to coordinates with respect to an ordered basis in W.</li>
</ul>
<h3 id="constructing-a-transformation-matrix">Constructing a Transformation Matrix</h3>
<ul>
<li><span class="math inline">\(T: \R^n \rightarrow \R^m\)</span></li>
<li>Choose basis <span class="math inline">\(v_1,...,v_n\)</span> for inputs and <span class="math inline">\(w_1,...,w_m\)</span> for outputs</li>
<li>For projection onto a line, choose v1 = line itself, v2 = vector perpendicular to line. Then for <span class="math inline">\(v = c_1v_1 + c_2v_2\)</span>, <span class="math inline">\(T(v)= c_1v_1 \text{ taking } (c_1, c_2) \rightarrow (c_1, 0)\)</span>. Then <span class="math inline">\(A = \left[\begin{array}{cc}1 &amp; 0 \\ 0&amp; 0\end{array}\right]\)</span></li>
<li>Easiest choice for a transformation matrix is the eigenvector basis, since this leads to transformation <span class="math inline">\(\Lambda\)</span></li>
<li>To find A given a basis, let the first column of A equal <span class="math inline">\(T(v_1)=a_{11}w_1 + ... + a_{m1}w_m\)</span>, the second column equal <span class="math inline">\(T(v_2)=a_{12}w_1 + ... + a_{m2}w_m\)</span>, etcâ¦</li>
</ul>
<h3 id="change-of-basis">Change of Basis</h3>
<ul>
<li>W matrix of new basis vectors as columns</li>
<li>To go to vector x in new basis from c in old basis, <span class="math inline">\(x = Wc\)</span></li>
<li>Transforming between bases is equivalent to similar matrices. M is a change of basis matrix in <span class="math inline">\(B = M^{-1}AM\)</span></li>
<li>Two matrices are equivalent if there exists matrices S and T st <span class="math inline">\(\tilde{\boldsymbol{A}}=\boldsymbol{T}^{-1} \boldsymbol{A} \boldsymbol{S}\)</span>. Similar matrices are always equivalent but not vice versa.</li>
</ul>
<h2 id="matrix-calculus">Matrix Calculus</h2>
<h3 id="univariate-calc-key-results">Univariate Calc Key Results</h3>
<ul>
<li>Difference quotient: <span class="math inline">\(\frac{\delta y}{\delta x}:=\frac{f(x+\delta x)-f(x)}{\delta x}\)</span></li>
<li>Derivative: <span class="math inline">\(\frac{\mathrm{d} f}{\mathrm{d} x}:=\lim _{h \rightarrow 0} \frac{f(x+h)-f(x)}{h}\)</span></li>
<li>Taylor Polynomial of degree n at <span class="math inline">\(x_0\)</span>: $Â T_{n}(x):=<em>{k=0}^{n} (x-x</em>{0})^{k} $ where <span class="math inline">\(f^{(k)}\left(x_{0}\right)\)</span> is the kth derivative at x0</li>
<li>Taylor Series at <span class="math inline">\(x_0\)</span>: <span class="math inline">\(T_{\infty}(x)=\sum_{k=0}^{\infty} \frac{f^{(k)}\left(x_{0}\right)}{k !}\left(x-x_{0}\right)^{k}\)</span></li>
<li>Differentiation Rules: <span class="math inline">\(\begin{aligned} &amp;\text { Product rule: } \quad(f(x) g(x))^{\prime}=f^{\prime}(x) g(x)+f(x) g^{\prime}(x)\\ &amp;\text { Quotient rule: } \quad\left(\frac{f(x)}{g(x)}\right)^{\prime}=\frac{f^{\prime}(x) g(x)-f(x) g^{\prime}(x)}{(g(x))^{2}}\\ &amp;\text { Sum rule: } \quad(f(x)+g(x))^{\prime}=f^{\prime}(x)+g^{\prime}(x)\\ &amp;\text { Chain rule: } \quad(g(f(x)))^{\prime}=(g \circ f)^{\prime}(x)=g^{\prime}(f(x)) f^{\prime}(x) \end{aligned}\)</span></li>
<li>You can think of <span class="math inline">\(\frac{d}{dx}\)</span> as an operator that maps a function of one parameter to another function - itâs distributive and we can just pull out constants</li>
</ul>
<h3 id="partial-differentiation-and-gradients">Partial Differentiation and Gradients</h3>
<ul>
<li>Partial Derivative: <span class="math inline">\(\frac{\partial f}{\partial x_{1}}=\lim _{h \rightarrow 0} \frac{f\left(x_{1}+h, x_{2}, \ldots, x_{n}\right)-f(x)}{h}\)</span> - collect them in a row vector</li>
<li>Gradient is simply a vector of partials of f.Â Each entry is a partial derivative with respect to a different variable: <span class="math inline">\(\nabla_{\boldsymbol{x}} f=\operatorname{grad} f=\frac{\mathrm{d} f}{\mathrm{d} \boldsymbol{x}}=\left[\frac{\partial f(\boldsymbol{x})}{\partial x_{1}} \quad \frac{\partial f(\boldsymbol{x})}{\partial x_{2}} \quad \cdots \quad \frac{\partial f(\boldsymbol{x})}{\partial x_{n}}\right] \in \mathbb{R}^{1 \times n}\)</span></li>
<li>The reason why we define the gradient vector as a row vector is twofold: First, we can consistently generalize the gradient to vector-valued functions f : Rn â Rm (then the gradient becomes a matrix). Second, we can immediately apply the multi-variate chain rule without paying attention to the dimension of the gradient.</li>
<li>Partial <span class="math inline">\(\text { Chain rule: } \quad \frac{\partial}{\partial \boldsymbol{x}}(g \circ f)(\boldsymbol{x})=\frac{\partial}{\partial \boldsymbol{x}}(g(f(\boldsymbol{x})))=\frac{\partial g}{\partial f} \frac{\partial f}{\partial \boldsymbol{x}}\)</span>
<ul>
<li><span class="math inline">\(\frac{\mathrm{d} f}{\mathrm{d} t}=\left[\begin{array}{ll} {\frac{\partial f}{\partial x_{1}}} &amp; {\frac{\partial f}{\partial x_{2}}} \end{array}\right]\left[\frac{\frac{\partial x_{1}(t)}{\partial t}}{\frac{\partial x_{2}(t)}{\partial t}}\right]=\frac{\partial f}{\partial x_{1}} \frac{\partial x_{1}}{\partial t}+\frac{\partial f}{\partial x_{2}} \frac{\partial x_{2}}{\partial t}\)</span> for x a function of t</li>
</ul></li>
<li>For <span class="math inline">\(f(x_1, x_2), \;x_1(s,t), \;x_2(s,t)\)</span>, the gradient given by <span class="math inline">\(\frac{\mathrm{d} f}{\mathrm{d}(s, t)}=\frac{\partial f}{\partial \boldsymbol{x}} \frac{\partial \boldsymbol{x}}{\partial(s, t)}=\left[\frac{\partial f}{\partial x_{1}} \quad \frac{\partial f}{\partial x_{2}}\right]\left[\begin{array}{ll} {\frac{\partial x_{1}}{\partial s}} &amp; {\frac{\partial x_{1}}{\partial t}} \\ {\frac{\partial x_{2}}{\partial s}} &amp; {\frac{\partial x_{2}}{\partial t}} \end{array}\right]\)</span></li>
</ul>
<h3 id="gradients-of-vector-valued-functions">Gradients of Vector Valued Functions</h3>
<ul>
<li>Function <span class="math inline">\(\boldsymbol{f}: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}\)</span> and vector X. Can take <span class="math inline">\(\boldsymbol{f}(\boldsymbol{x})=\left[\begin{array}{c} {f_{1}(\boldsymbol{x})} \\ {\vdots} \\ {f_{m}(\boldsymbol{x})} \end{array}\right] \in \mathbb{R}^{m}\)</span></li>
<li>Partial derivative of a vector valued function <span class="math inline">\(\frac{\partial \boldsymbol{f}}{\partial x_{i}}=\left[\begin{array}{c} {\frac{\partial f_{1}}{\partial x_{i}}} \\ {\vdots} \\ {\frac{\partial f_{m}}{\partial x_{i}}} \end{array}\right]\)</span></li>
<li>The Jacobian is the collection of all first order partial derivatives of vector valued function f: <span class="math inline">\(\boldsymbol{J}=\nabla_{x} \boldsymbol{f}=\frac{\mathrm{d} \boldsymbol{f}(\boldsymbol{x})}{\mathrm{d} \boldsymbol{x}}=\left[\frac{\partial \boldsymbol{f}(\boldsymbol{x})}{\partial x_{1}} \quad \cdots \quad \frac{\partial \boldsymbol{f}(\boldsymbol{x})}{\partial x_{n}}\right] = =\left[\begin{array}{ccc} {\frac{\partial f_{1}(\boldsymbol{x})}{\partial x_{1}}} &amp; {\cdots} &amp; {\frac{\partial f_{1}(\boldsymbol{x})}{\partial x_{n}}} \\ {\vdots} &amp; {} &amp; {\vdots} \\ {\frac{\partial f_{m}(\boldsymbol{x})}{\partial x_{1}}} &amp; {\cdots} &amp; {\frac{\partial f_{m}(\boldsymbol{x})}{\partial x_{n}}} \end{array}\right]\)</span>. J is an m x n matrix</li>
<li>J can be seen as a basis change matrix, taking the determinant is the area of a parallelogram. The change in the absolute value of the determinant of J describes how the area changes under the basis change.</li>
<li>The calculus approach gives us the familiar Jacobian <span class="math inline">\(\boldsymbol{J}=\left[\begin{array}{ll} {\frac{\partial y_{1}}{\partial x_{1}}} &amp; {\frac{\partial y_{1}}{\partial x_{2}}} \\ {\frac{\partial y_{2}}{\partial x_{1}}} &amp; {\frac{\partial y_{2}}{\partial x_{2}}} \end{array}\right]\)</span> for a mapping of y in terms of x. The absolute value of the Jacobian determinant |det(J)| is the factor by which areas or volumes are scaled when coordinates are transformed. These transformations are extremely relevant in ML learning in the context of training deep neural networks using the reparametrization trick, also called infinite perturbation analysis.</li>
<li>Gradients in least squares: for <span class="math inline">\(\boldsymbol{y}=\boldsymbol{\Phi} \boldsymbol{\theta}\)</span>, <span class="math inline">\(\begin{aligned} &amp;L(e):=\|e\|^{2},\; e(\boldsymbol{\theta}):=\boldsymbol{y}-\boldsymbol{\Phi} \boldsymbol{\theta} \end{aligned}\)</span>, we seek <span class="math inline">\(\frac{\partial L}{\partial \boldsymbol{\theta}}\)</span>. We use the chain rule <span class="math inline">\(\frac{\partial L}{\partial \boldsymbol{\theta}}=\frac{\partial L}{\partial e} \frac{\partial e}{\partial \theta}\)</span>. Then <span class="math inline">\(\frac{\partial L}{\partial e}=2 e^{\tau}\)</span> since <span class="math inline">\(\|e\|^{2}=e^{T} e\)</span> and <span class="math inline">\(\frac{\partial e}{\partial \theta}=-\Phi \in \mathbf{R}^{N \times D}\)</span>. In total: <span class="math inline">\(\frac{\partial L}{\partial \theta}=-2 e^{\top} \Phi \stackrel{(5, .77)}{=}-\underbrace{2\left(\boldsymbol{y}^{\top}-\boldsymbol{\theta}^{\top} \boldsymbol{\Phi}^{\top}\right)}_{1 \times N} \underbrace{\Phi}_{N \times D} \in \mathbf{R}^{1 \times D}\)</span></li>
</ul>
<h3 id="gradients-of-matrices">Gradients of Matrices</h3>
<ul>
<li>For details, see <a href="https://explained.ai/matrix-calculus/#sec4.3">explained.ai</a></li>
<li>When we move from derivatives of one function to derivatives of many functions, we move from the world of vector calculus to matrix calculus.</li>
<li>Gradient vectors organize all of the partial derivatives for a specific scalar function. Say we have functions <span class="math inline">\(f(x, y)=3 x^{2} y,\;g(x, y)=2 x+y^{8}\)</span> If we have two functions, we can also organize their gradients into a matrix by stacking the gradients - this gives us a Jacobian matrix. <span class="math inline">\(J=\left[\begin{array}{c} {\nabla f(x, y)} \\ {\nabla g(x, y)} \end{array}\right]=\left[\begin{array}{ll} {\frac{\partial f(x, y)}{\partial x}} &amp; {\frac{\partial f(x, y)}{\partial y}} \\ {\frac{\partial g(x, y)}{\partial x}} &amp; {\frac{\partial g(x, y)}{\partial y}} \end{array}\right]=\left[\begin{array}{cc} {6 y x} &amp; {3 x^{2}} \\ {2} &amp; {8 y^{7}} \end{array}\right]\)</span>. Note this layout is the <strong>numerator layout</strong>. Some in ML use the denominator layout, which is the transpose: <span class="math inline">\(\left[\begin{array}{cc} {6 y x} &amp; {2} \\ {3 x^{2}} &amp; {8 y^{7}} \end{array}\right]\)</span></li>
<li>With multiple scalar-valued functions, we can combine them all into a vector just like we did with the parameters. Let y = f(x) be a vector of m scalar-values functions that each take a vector x of length n.Â From our prior examples: $ y_{1}=f_{1}(x)=3 x_{1}^{2} x_{2},;y_{2}=f_{2}()=2 x_{1}+x_{2}^{8}$</li>
<li>Generally, the Jacobian is the collection of all m x n possible partial derivatives, ie. a stack of m gradients wrt x: <span class="math inline">\(\frac{\partial \mathbf{y}}{\partial \mathbf{x}}=\left[\begin{array}{c} {\nabla f_{1}(\mathbf{x})} \\ {\nabla f_{2}(\mathbf{x})} \\ {\cdots} \\ {\nabla f_{m}(\mathbf{x})} \end{array}\right]=\left[\begin{array}{c} {\frac{\partial}{\partial x} f_{1}(\mathbf{x})} \\ {\frac{\partial}{\partial \mathbf{x}} f_{2}(\mathbf{x})} \\ {\cdots} \\ {\frac{\partial}{\partial \mathbf{x}} f_{m}(\mathbf{x})} \end{array}\right]=\left[\begin{array}{ccc} {\frac{\partial}{\partial x_{1}} f_{1}(\mathbf{x})} &amp; {\frac{\partial}{\partial x_{2}} f_{1}(\mathbf{x})} &amp; {\cdots} &amp; {\frac{\partial}{\partial x_{n}} f_{1}(\mathbf{x})} \\ {\frac{\partial}{\partial x_{1}} f_{2}(\mathbf{x})} &amp; {\frac{\partial}{\partial x_{2}} f_{2}(\mathbf{x})} &amp; {\cdots} &amp; {\frac{\partial}{\partial x_{n}} f_{2}(\mathbf{x})} \\ {\cdots} &amp; {} &amp; {\cdots} &amp; {} \\ {\frac{\partial}{\partial x_{1}} f_{m}(\mathbf{x})} &amp; {\frac{\partial}{\partial x_{2}} f_{m}(\mathbf{x})}&amp; {\cdots} &amp; { \frac{\partial}{\partial x_{n}} f_{m}(\mathbf{x})} \end{array}\right]\)</span></li>
<li>We often have functions combined by element-wise binary operators (apply an operator the first item of ecch vector to get the first item of the output, then the second, etc). We are left with an ugly Jacobian applied to <span class="math inline">\(\mathbf{y}=\mathbf{f}(\mathbf{w}) \bigcirc \mathbf{g}(\mathbf{x})\)</span> wrt the x vector. However, we are left with a diagonal matrix, since when i not equal j, <span class="math inline">\(\frac{\partial}{\partial w_{j}}\left(f_{i}(\mathbf{w}) \bigcirc g_{i}(\mathbf{x})\right)=0\)</span> since taking derivatives of constants.</li>
<li>Scalars: changing vectors by a scalar is really an element-wise operation. For scalar, wrt to the variable z, we get a vector <span class="math inline">\(\frac{\partial}{\partial z}\left(f_{i}\left(x_{i}\right)+g_{i}(z)\right)=\frac{\partial\left(x_{i}+z\right)}{\partial z}=\frac{\partial x_{i}}{\partial z}+\frac{\partial z}{\partial z}=0+1=1\)</span>. Alternatively, wrt x, we get a diagonal Jacobian with elements <span class="math inline">\(\frac{\partial}{\partial x_{i}}\left(f_{i}\left(x_{i}\right) \otimes g_{i}(z)\right)=x_{i} \frac{\partial z}{\partial x_{i}}+z \frac{\partial x_{i}}{\partial x_{i}}=0+z=z\)</span></li>
<li>Sums: we often need to sum over results, and we can move the sum outside of the derivative. For <span class="math inline">\(y=\operatorname{sum}(\mathbf{f}(\mathbf{x}))=\sum_{i=1}^{n} f_{i}(\mathbf{x})\)</span>, we can get gradient <span class="math inline">\(\nabla y=\left[\sum_{i} \frac{\partial f(x)}{\partial x_{1}}, \sum_{i} \frac{\partial f(x)}{\partial x_{2}}, \ldots, \sum_{i} \frac{\partial f(x)}{\partial x_{n}}\right]=\left[\sum_{i} \frac{\partial x_{i}}{\partial x_{i}}, \sum_{i} \frac{\partial x_{i}}{\partial x_{i}}, \ldots, \Sigma_{i} \frac{\partial x}{\partial x_{i}}\right] = \left[\frac{\partial x_{1}}{\partial x_{1}}, \frac{\partial x_{2}}{\partial x_{2}}, \ldots, \frac{\partial x_{n}}{\partial x_{n}}\right]=[1,1, \ldots, 1]=\overrightarrow{\mathrm{I}}^{T}\)</span> since <span class="math inline">\(\frac{\partial}{\partial x_{j}} x_{i}=0 \text { for } j \neq i\)</span>.</li>
</ul>
<h5 id="chain-rules">Chain Rules</h5>
<ul>
<li>Forward differentiation from x to y: <span class="math inline">\(\frac{d y}{d x}=\frac{d u}{d x} \frac{d y}{d u}\)</span>. Backward differentiation from y to x: <span class="math inline">\(\frac{d y}{d x}=\frac{d y}{d u} \frac{d u}{d x}\)</span></li>
<li>When x affects y through a single data flow path in nested functions, we simply introduce intermediate variables, compute derivatives wrt each, then combine using the chain rule, eg <span class="math inline">\(\frac{d y}{d x}=\frac{d u_{4}}{d x}=\frac{d u_{4}}{d u_{3}} \frac{d u_{3}}{d u_{2}} \frac{d u_{2}}{d u_{1}} \frac{d u_{1}}{d x}\)</span>. With an expression like <span class="math inline">\(f(x) = x + x^2\)</span>, we need a different technique since x affects y through 2 different pathways.</li>
<li>We use the law of total derivatives - to compute derivative we need to sum up all possible contributions from changes in x to the change in y. <strong>Single variable total derivative chain rule</strong> that assumes all variables may be codependent: <span class="math inline">\(\frac{\partial f\left(x, u_{1}, \ldots, u_{n}\right)}{\partial x}=\frac{\partial f}{\partial x}+\frac{\partial f}{\partial u_{1}} \frac{\partial u_{1}}{\partial x}+\frac{\partial f}{\partial u_{2}} \frac{\partial u_{2}}{\partial x}+\ldots+\frac{\partial f}{\partial u_{n}} \frac{\partial u_{n}}{\partial x}=\frac{\partial f}{\partial x}+\sum_{i=1}^{n} \frac{\partial f}{\partial u_{i}} \frac{\partial u_{i}}{\partial x}\)</span></li>
<li>The total derivative is adding terms because it represents a weighted sum of all x contributions to the change in y.</li>
<li>Vector chain rule: We can take the single variable chain rule <span class="math inline">\(\frac{d}{d x} f(g(x))=\frac{d f}{d g} \frac{d g}{d x}\)</span> and convert to a vector rule <span class="math inline">\(\frac{\partial}{\partial x} \mathbf{f}(\mathrm{g}(x))=\frac{\partial \mathrm{f}}{\partial \mathrm{g}} \frac{\partial \mathrm{g}}{\partial x} =\left[\begin{array}{ll} {\frac{\partial f_{1}}{\partial g_{1}}} &amp; {\frac{\partial f_{1}}{\partial g_{2}}} \\ {\frac{\partial f_{2}}{\partial g_{1}}} &amp; {\frac{\partial f_{2}}{\partial g_{2}}} \end{array}\right]\left[\begin{array}{l} {\frac{\partial g_{1}}{\partial x}} \\ {\frac{\partial g_{2}}{\partial x}} \end{array}\right]\)</span> . To broaden to multiple parameters, vector x, we now multiply two full matrix Jacobians: <span class="math inline">\(\frac{\partial}{\partial \mathbf{x}} \mathbf{f}(\mathbf{g}(\mathbf{x}))=\left[\begin{array}{cccc} {\frac{\partial f_{1}}{\partial g_{1}}} &amp; {\frac{\partial f_{1}}{\partial g_{2}}} &amp; {\cdots} &amp; {\frac{\partial f_{1}}{\partial g_{k}}} \\ {\frac{\partial f_{2}}{\partial g_{1}}} &amp; {\frac{\partial f_{2}}{\partial g_{2}}} &amp; {\cdots} &amp; {\frac{\partial f_{2}}{\partial g_{k}}} \\ {\frac{\partial f_{m}}{\partial g_{1}}} &amp; {\frac{\partial f_{m}}{\partial g_{2}}} &amp; {\cdots} &amp; {\frac{\partial f_{m}}{\partial g_{k}}} \end{array}\right] \left[\begin{array}{cccc} {\frac{\partial g_{1}}{\partial x_{1}}} &amp; {\frac{\partial g_{1}}{\partial x_{2}}} &amp; {\cdots} &amp; {\frac{\partial g_{1}}{\partial x_{n}}} \\ {\frac{\partial g_{2}}{\partial x_{1}}} &amp; {\frac{\partial g_{2}}{\partial x_{2}}} &amp; {\cdots} &amp; {\frac{\partial g_{2}}{\partial x_{n}}} \\ {\frac{\partial g_{k}}{\partial x_{1}}} &amp; {\frac{\partial g_{k}}{\partial x_{2}}} &amp; {\cdots} &amp; {\frac{\partial g_{k}}{\partial x_{n}}} \end{array}\right]\)</span></li>
<li>Most often, the Jacobian reduces to a diagonal matrix whose elements are the single variable chain rule values</li>
<li>A summary to get to the Jacobian: <img src="/Users/spencerbraun/Documents/Notes/Stanford/YBxJ4nuULTKdl1cRaDTxGz2KMAOxhaacEB6dvfdGDpk.original.fullsize.png" alt="Jacobian" /></li>
</ul>
<h2 id="applications">Applications</h2>
<h3 id="low-rank-matrix-approximations">Low Rank Matrix Approximations</h3>
<ul>
<li>The following are equivalent definitions for the rank of a matrix B to be k
<ul>
<li>The largest linearly independent subset of columns</li>
<li>The largest linearly independent subset of rows</li>
<li>B can written as, or âfactored into,â the product of long and skinny (nÃk) matrix <span class="math inline">\(Y_k\)</span> and a short and long (kÃd) matrix <span class="math inline">\(Z_k^T\)</span>. Think outer product</li>
</ul></li>
<li>Idea is to approximate our matrix with a matrix of rank k - useful for compression or denoising.</li>
<li>For every n x d A with rank target k and given a rank-k n x d matrix B, <span class="math inline">\(\left\|\mathbf{A}-\mathbf{A}_{k}\right\|_{F} \leq\|\mathbf{A}-\mathbf{B}\|_{F}\)</span></li>
<li>For <span class="math inline">\(X = A - A_k,\;||X||_F\)</span> measures the discrepancy between A and its approximation. Want to find the <span class="math inline">\(A_k\)</span> that minimizes this distance.</li>
</ul>
<h5 id="using-svd">Using SVD</h5>
<ul>
<li><span class="math inline">\(\mathbf{A}=\mathbf{U S V}^{T}\)</span>, for U nxn orthogonal matrix, V dxd orthogonal matrix, S nxd matrix of nonnegative entries with diagonal entries sorted from high to low. Columns of U are left singular values of A, V are right singular values of A. Entries of S are singular values of A.</li>
<li>Choosing a rank-k matrix boils down to choosing a set of k basis vectors. What vectors to choose? The SVD gives us a representation of A as a linear combination of sets of vectors ordered by importance!</li>
<li>Given n x d matrix A and target rank k, we do the following
<ul>
<li>Compute SVD <span class="math inline">\(A = USV^T\)</span>. Keep only the top k right singular vectors: set <span class="math inline">\(V^T_k\)</span> equal to the first k rows of <span class="math inline">\(V^T\)</span></li>
<li>Keep only the top k left singular vectors: first k columns of U</li>
<li>Keep only the top k singular values: first k rows / columns of S, the k largest singular values of A</li>
</ul></li>
<li>Low rank approximation is then <span class="math inline">\(\mathbf{A}_{k}=\mathbf{U}_{k} \mathbf{S}_{k} \mathbf{V}_{k}^{T}\)</span> . This now takes <span class="math inline">\(O(k(n+d))\)</span> space to store instead of <span class="math inline">\(O(nd)\)</span></li>
<li>This is akin to approximating A in terms of k âconceptsâ where the singular values express the signal strength of these concepts, rows of V^T and columns of U express the canonical row/column associated with each concept and rows of U and cols of V^T express each row / column of A as a linear combination of the canonical rows</li>
<li>Alternate approach: From U and V can construct rank-1 matrices via <span class="math inline">\(\boldsymbol{A}_{i}:=\boldsymbol{u}_{i} \boldsymbol{v}_{i}^{\top}\)</span>. A matrix of rank r can be written as a sum of rank 1 matrices. Construct rank-1 matrices using each singular value, then consider a rank-2 (etc) matrix <span class="math inline">\(\widehat{\boldsymbol{A}}(2)=\sigma_{1} \boldsymbol{A}_{1}+\sigma_{2} \boldsymbol{A}_{2}\)</span>. Stop the combination when we have a close enough approximation.</li>
</ul>
<h5 id="choosing-k">Choosing K</h5>
<ul>
<li>Ideally, guidance from eigenvalues of <span class="math inline">\(A^TA\)</span> or singular values of A. If top few are big and rest are small, cut off is relatively obvious</li>
<li>Often choose k st the sum of the top k eigenvalues is at least c times as big as the sum of othe eigenvalues.</li>
<li>The effect of small eigenvalues on matrix products is small. Thus, it seems plausible that replacing these small eigenvalues by zero will not substantially alter the product</li>
</ul>
<h5 id="application-fill-in-missing-values">Application: Fill in missing values</h5>
<ul>
<li>A is a matrix of Netflix customers and movie ratings. A reasonable assumption that makes the problemmore tractable is that the matrix to be recovered is well-approximated by a low-rank matrix.</li>
<li>If there arenât too many missing entries, and if the matrix to be recoveredis approximately low rank, then the following application of the SVD can yield a good guessas to the missing entries</li>
<li>Fill in missing entries with suitable default values to obtain a matrix <span class="math inline">\(\hat{A}\)</span> then compute the best rank-k approximation of <span class="math inline">\(\hat{A}\)</span></li>
</ul>
<h3 id="markov-matrices">Markov Matrices</h3>
<ul>
<li>Example: <span class="math inline">\(A = \left[\begin{array}{ccc}.1 &amp; .01 &amp; .3 \\ .2 &amp; .99 &amp; .3 \\ .7 &amp; 0 &amp; .4 \end{array}\right]\)</span></li>
<li>All entries <span class="math inline">\(\geq 0\)</span> and all columns add to 1</li>
<li>Property 2 guarantees 1 is an eigenvalue, all other eigenvals must be less than 1</li>
<li>Taking <span class="math inline">\(A - 1\lambda\)</span> creates singular matrix, cols add to 0.</li>
</ul>
<h3 id="fast-fourier-transform">Fast Fourier Transform</h3>
<h3 id="differential-equations">Differential Equations</h3>
<h3 id="matrix-exponentials">Matrix Exponentials</h3>
</body>
</html>
