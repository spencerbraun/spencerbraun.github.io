<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>PyTorch on Spencer Braun</title>
    <link>https://spencerbraun.github.io/tags/pytorch/</link>
    <description>Recent content in PyTorch on Spencer Braun</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 28 Apr 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://spencerbraun.github.io/tags/pytorch/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Socratic Models - AI Ensembles</title>
      <link>https://spencerbraun.github.io/posts/socratic-models-applications/</link>
      <pubDate>Thu, 28 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://spencerbraun.github.io/posts/socratic-models-applications/</guid>
      <description>The period of 2018-2020 was a critical time for progress in natural language processing. On the back of Transformers, a wave of value was created with products like text completion, sentiment analysis, entity recognition, and writing improvement tools. The process for creating these technologies would be familiar to data scientists across domains: collect and label data, train and fine-tune models, test different architectures, and optimize hyperparameters.
For those close to NLP research and development, the last year has presented a clear break from this incremental progress.</description>
    </item>
    
    <item>
      <title>Selectively Editable Language Models</title>
      <link>https://spencerbraun.github.io/posts/selectively-editable-language-models/</link>
      <pubDate>Wed, 14 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://spencerbraun.github.io/posts/selectively-editable-language-models/</guid>
      <description>The post belows explains a project in editing language models I completed as part of Stanford’s course in natural language processing. If you want to skip to the point, the report and code are freely available.
Background: Taming Large Language Models Large language models have been an area of exciting development in the last few years, and we now have applications and entire companies built around their improving performance and utility.</description>
    </item>
    
    <item>
      <title>Chinese Word Segmentation: Classic and Modern Methods</title>
      <link>https://spencerbraun.github.io/posts/chinese-segmentation-classic-and-modern-methods/</link>
      <pubDate>Sat, 10 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://spencerbraun.github.io/posts/chinese-segmentation-classic-and-modern-methods/</guid>
      <description>Part of the fun of NLP is the diversity of language structures that models must consider. Tools that are built in a monolingual context can easily fail to achieve more global utility. If you just work in English, you may find word embeddings fun and useful, but the German Komposita don’t fit neatly into our English-centric boxes. Similarly, once we leave the Latin alphabet, we acquire all sorts of new challenges.</description>
    </item>
    
    <item>
      <title>Unsupervised Text Style Transfer with Deep Learning</title>
      <link>https://spencerbraun.github.io/posts/generative_text_style_transfer/</link>
      <pubDate>Mon, 08 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://spencerbraun.github.io/posts/generative_text_style_transfer/</guid>
      <description>Natural Language Processing (NLP) is one area of deep learning that continues to make substantial, and often surprising, progress while also adding value to existing businesses. Supervised tasks like neural machine translation produce high quality, real-time results and Gmail&amp;rsquo;s predictive text feature often feels like magic. I have been most interested in recent applications of generative text models, such as using GPT-2 to play chess, write poetry, or create custom games.</description>
    </item>
    
  </channel>
</rss>
